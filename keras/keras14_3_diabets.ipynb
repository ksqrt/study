{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "x = dataset.data\n",
    "y = dataset.target\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 사이킥 런의 칼럼 조회기능\n",
    "print(dataset.feature_names)\n",
    "print(len(dataset.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사이킥런의 데이터 요약\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델구성\n",
    "# 실습 train 0.7 이상\n",
    "# 평가지표 R2 : 0.8 이상으로 뽑아내보기 / RMSE 사용\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,\n",
    "    train_size=0.7,\n",
    "    shuffle = True,\n",
    "    random_state= 123\n",
    ")\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,881\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "number1 = randrange(1,10,1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10,input_dim = 10),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 151.8502\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 74.0979\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 56.9586\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 54.4043\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 49.2971\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 48.0678\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 47.6758\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 46.4775\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 46.5343\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.4253\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 45.4476\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 45.9825\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2036\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.7419\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.3857\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9862\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.4286\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9016\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.8930\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.6421\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 44.7803\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.4282\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6145\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.2502\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5158\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8690\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.3287\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8354\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9190\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2439\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.3076\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.7575\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.2721\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4749\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.3050\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.2203\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.4959\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.8584\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.7593\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.6951\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.9976\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4810\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.9859\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.5011\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.7839\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.0300\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.3254\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.7587\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.3091\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.9853\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8478\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.0124\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.9977\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9123\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.0915\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9296\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.9054\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5139\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.9020\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2373\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2658\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.8764\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.2084\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.6407\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8858\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.6720\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.8281\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2995\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8530\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.0224\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.9597\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.0968\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.0707\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9050\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.4440\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.9504\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4299\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.5016\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 170us/step - loss: 40.5016\n",
      "5/5 [==============================] - 0s 997us/step - loss: 43.9608\n",
      "loss :  43.960784912109375\n",
      "5/5 [==============================] - 0s 748us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[136.37285 ]\n",
      " [129.40178 ]\n",
      " [241.45726 ]\n",
      " [169.87386 ]\n",
      " [132.76726 ]\n",
      " [126.32632 ]\n",
      " [268.86264 ]\n",
      " [ 89.1562  ]\n",
      " [ 77.70249 ]\n",
      " [118.0905  ]\n",
      " [111.445595]\n",
      " [169.24828 ]\n",
      " [138.47615 ]\n",
      " [208.28502 ]\n",
      " [237.16747 ]\n",
      " [155.74771 ]\n",
      " [105.04868 ]\n",
      " [162.5282  ]\n",
      " [173.41885 ]\n",
      " [185.73846 ]\n",
      " [168.93639 ]\n",
      " [239.36183 ]\n",
      " [131.5496  ]\n",
      " [ 71.17714 ]\n",
      " [ 97.32807 ]\n",
      " [191.97755 ]\n",
      " [101.779655]\n",
      " [102.65325 ]\n",
      " [136.02351 ]\n",
      " [170.75635 ]\n",
      " [ 98.79686 ]\n",
      " [260.20123 ]\n",
      " [220.04922 ]\n",
      " [208.64705 ]\n",
      " [195.32362 ]\n",
      " [ 94.303856]\n",
      " [ 67.89863 ]\n",
      " [108.25468 ]\n",
      " [233.24109 ]\n",
      " [ 92.42623 ]\n",
      " [195.64355 ]\n",
      " [ 81.66322 ]\n",
      " [115.34067 ]\n",
      " [130.62509 ]\n",
      " [105.96218 ]\n",
      " [227.57211 ]\n",
      " [101.75573 ]\n",
      " [ 91.99811 ]\n",
      " [ 71.81013 ]\n",
      " [236.58704 ]\n",
      " [ 89.74685 ]\n",
      " [ 84.61264 ]\n",
      " [181.69514 ]\n",
      " [196.51477 ]\n",
      " [157.78345 ]\n",
      " [195.6226  ]\n",
      " [234.38162 ]\n",
      " [156.94548 ]\n",
      " [194.83192 ]\n",
      " [187.08165 ]\n",
      " [122.63703 ]\n",
      " [118.08701 ]\n",
      " [241.2292  ]\n",
      " [210.38972 ]\n",
      " [184.2188  ]\n",
      " [ 77.82269 ]\n",
      " [ 72.830605]\n",
      " [154.80443 ]\n",
      " [115.28144 ]\n",
      " [ 84.8791  ]\n",
      " [105.21403 ]\n",
      " [ 44.936455]\n",
      " [156.29457 ]\n",
      " [201.75752 ]\n",
      " [129.7717  ]\n",
      " [153.66618 ]\n",
      " [145.60977 ]\n",
      " [127.99128 ]\n",
      " [104.58332 ]\n",
      " [ 85.227715]\n",
      " [ 84.96871 ]\n",
      " [142.7736  ]\n",
      " [141.13907 ]\n",
      " [237.90575 ]\n",
      " [328.56625 ]\n",
      " [151.72498 ]\n",
      " [201.80043 ]\n",
      " [ 83.285736]\n",
      " [111.278114]\n",
      " [122.79131 ]\n",
      " [153.10765 ]\n",
      " [ 60.23421 ]\n",
      " [167.3395  ]\n",
      " [158.10414 ]\n",
      " [101.72462 ]\n",
      " [276.9998  ]\n",
      " [149.38153 ]\n",
      " [224.18593 ]\n",
      " [132.43182 ]\n",
      " [138.83907 ]\n",
      " [ 71.5499  ]\n",
      " [ 61.921402]\n",
      " [110.753105]\n",
      " [167.99156 ]\n",
      " [190.62527 ]\n",
      " [ 82.67647 ]\n",
      " [134.57681 ]\n",
      " [159.5343  ]\n",
      " [178.42775 ]\n",
      " [251.7561  ]\n",
      " [ 88.183136]\n",
      " [129.59717 ]\n",
      " [119.455864]\n",
      " [ 73.90041 ]\n",
      " [ 95.33068 ]\n",
      " [ 81.26571 ]\n",
      " [195.92891 ]\n",
      " [ 85.74433 ]\n",
      " [166.67561 ]\n",
      " [177.60988 ]\n",
      " [142.55547 ]\n",
      " [140.6126  ]\n",
      " [137.58655 ]\n",
      " [109.55838 ]\n",
      " [192.60533 ]\n",
      " [220.32985 ]\n",
      " [244.04973 ]\n",
      " [194.17578 ]\n",
      " [ 67.05253 ]\n",
      " [294.1083  ]\n",
      " [103.41086 ]\n",
      " [142.49306 ]\n",
      " [132.93126 ]]\n",
      "=================\n",
      "R2 :  0.5045740043977363\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.5712\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1778\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.3762\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.5628\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4809\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0288\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0883\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.8158\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.3882\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6494\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4829\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3969\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.9704\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2397\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.8037\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9294\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0152\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3736\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.1945\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4021\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3406\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6378\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.5645\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6851\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8393\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3613\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3333\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.9708\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3161\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7253\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.5962\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.9038\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.7229\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.3914\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.1756\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.4332\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.7325\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.3300\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.2877\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2565\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0607\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3010\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2077\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3517\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8121\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.5695\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.5436\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3685\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9378\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0076\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6044\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3863\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.0039\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3854\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0284\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6080\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9416\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.9034\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.5939\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3048\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.2555\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.6325\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.9519\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6978\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0775\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.2477\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6040\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8671\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8261\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2290\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.8554\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.2765\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0538\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.6476\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7250\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1171\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5662\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2484\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 41.2484\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 45.5988\n",
      "loss :  45.59879684448242\n",
      "5/5 [==============================] - 0s 748us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[131.79234 ]\n",
      " [128.31633 ]\n",
      " [250.58232 ]\n",
      " [172.08354 ]\n",
      " [134.77325 ]\n",
      " [148.13322 ]\n",
      " [291.35095 ]\n",
      " [ 93.7525  ]\n",
      " [ 77.47102 ]\n",
      " [153.849   ]\n",
      " [131.91605 ]\n",
      " [174.09756 ]\n",
      " [141.8468  ]\n",
      " [216.89131 ]\n",
      " [241.12619 ]\n",
      " [157.39952 ]\n",
      " [110.42795 ]\n",
      " [184.62083 ]\n",
      " [175.28267 ]\n",
      " [191.74496 ]\n",
      " [176.03862 ]\n",
      " [256.0149  ]\n",
      " [137.9377  ]\n",
      " [ 61.901985]\n",
      " [ 96.55304 ]\n",
      " [200.06827 ]\n",
      " [113.1806  ]\n",
      " [102.3141  ]\n",
      " [138.05518 ]\n",
      " [165.90288 ]\n",
      " [108.68326 ]\n",
      " [272.602   ]\n",
      " [217.01399 ]\n",
      " [223.54472 ]\n",
      " [196.45921 ]\n",
      " [ 97.4395  ]\n",
      " [ 61.70639 ]\n",
      " [114.28677 ]\n",
      " [248.17796 ]\n",
      " [ 93.21002 ]\n",
      " [198.53783 ]\n",
      " [ 77.25865 ]\n",
      " [118.24492 ]\n",
      " [122.60906 ]\n",
      " [118.16646 ]\n",
      " [240.42635 ]\n",
      " [107.44944 ]\n",
      " [ 99.31792 ]\n",
      " [ 72.30588 ]\n",
      " [252.84656 ]\n",
      " [117.03189 ]\n",
      " [ 81.97836 ]\n",
      " [198.2823  ]\n",
      " [208.92345 ]\n",
      " [183.21239 ]\n",
      " [198.19984 ]\n",
      " [252.77483 ]\n",
      " [164.34874 ]\n",
      " [200.22212 ]\n",
      " [193.20251 ]\n",
      " [115.55092 ]\n",
      " [132.71169 ]\n",
      " [259.87732 ]\n",
      " [225.27356 ]\n",
      " [192.08218 ]\n",
      " [103.4434  ]\n",
      " [ 66.87799 ]\n",
      " [171.19829 ]\n",
      " [139.67432 ]\n",
      " [ 74.085205]\n",
      " [ 94.75008 ]\n",
      " [ 50.475594]\n",
      " [173.5854  ]\n",
      " [206.50063 ]\n",
      " [141.35143 ]\n",
      " [155.42505 ]\n",
      " [132.47977 ]\n",
      " [129.65923 ]\n",
      " [107.27737 ]\n",
      " [ 86.91623 ]\n",
      " [ 89.133316]\n",
      " [146.80579 ]\n",
      " [166.73888 ]\n",
      " [247.16765 ]\n",
      " [347.30487 ]\n",
      " [166.77165 ]\n",
      " [210.25865 ]\n",
      " [ 75.61888 ]\n",
      " [113.13759 ]\n",
      " [143.47595 ]\n",
      " [155.16522 ]\n",
      " [ 46.916054]\n",
      " [174.62721 ]\n",
      " [168.39801 ]\n",
      " [119.26259 ]\n",
      " [298.43146 ]\n",
      " [164.73538 ]\n",
      " [240.89165 ]\n",
      " [138.24487 ]\n",
      " [135.65274 ]\n",
      " [ 60.459892]\n",
      " [ 65.166565]\n",
      " [111.28166 ]\n",
      " [184.38521 ]\n",
      " [201.26189 ]\n",
      " [ 83.50003 ]\n",
      " [133.2227  ]\n",
      " [164.51958 ]\n",
      " [187.22017 ]\n",
      " [273.9842  ]\n",
      " [ 75.75595 ]\n",
      " [137.02086 ]\n",
      " [131.84837 ]\n",
      " [ 83.589775]\n",
      " [104.180214]\n",
      " [ 94.38829 ]\n",
      " [205.26866 ]\n",
      " [110.691635]\n",
      " [181.98915 ]\n",
      " [199.46144 ]\n",
      " [144.77939 ]\n",
      " [139.1814  ]\n",
      " [151.35974 ]\n",
      " [116.44818 ]\n",
      " [202.1268  ]\n",
      " [228.16154 ]\n",
      " [262.05862 ]\n",
      " [201.34859 ]\n",
      " [ 55.458527]\n",
      " [295.88605 ]\n",
      " [111.12782 ]\n",
      " [172.5461  ]\n",
      " [167.39494 ]]\n",
      "=================\n",
      "R2 :  0.4615214481253832\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8040\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6320\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.2250\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6795\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9349\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0476\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.2332\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4412\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7125\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.8569\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7355\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5099\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8591\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.6976\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9448\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0109\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8066\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3310\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4944\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8529\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9344\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2248\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0147\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.5638\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9319\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3109\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8025\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.2327\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0167\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3993\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0158\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.3702\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8601\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6749\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.1421\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8778\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8053\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.5198\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.2135\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3010\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9345\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4650\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.5923\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7591\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.2344\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.7535\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.5279\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.2920\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5277\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3186\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9939\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1187\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2722\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2073\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2779\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.6242\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3010\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9845\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.4200\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.4127\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9660\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.3928\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7192\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5102\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6045\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1998\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0396\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.1558\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7368\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5395\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.5135\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.7501\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4438\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7001\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.2001\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.2206\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8236\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.1610\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 36.1610\n",
      "5/5 [==============================] - 0s 997us/step - loss: 46.9971\n",
      "loss :  46.99711990356445\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[ 99.29918 ]\n",
      " [101.20592 ]\n",
      " [199.50716 ]\n",
      " [133.89635 ]\n",
      " [107.90474 ]\n",
      " [127.66893 ]\n",
      " [244.97298 ]\n",
      " [ 74.36948 ]\n",
      " [ 66.572136]\n",
      " [151.54784 ]\n",
      " [117.02349 ]\n",
      " [135.91757 ]\n",
      " [108.91222 ]\n",
      " [173.05762 ]\n",
      " [205.47789 ]\n",
      " [132.90865 ]\n",
      " [ 82.94009 ]\n",
      " [156.0651  ]\n",
      " [134.15733 ]\n",
      " [155.06757 ]\n",
      " [139.99922 ]\n",
      " [203.48055 ]\n",
      " [106.37181 ]\n",
      " [ 41.44172 ]\n",
      " [ 67.48677 ]\n",
      " [166.68929 ]\n",
      " [ 87.89654 ]\n",
      " [ 83.80463 ]\n",
      " [119.64012 ]\n",
      " [127.84241 ]\n",
      " [ 84.98119 ]\n",
      " [216.39407 ]\n",
      " [164.61613 ]\n",
      " [186.08902 ]\n",
      " [147.46892 ]\n",
      " [ 85.12265 ]\n",
      " [ 41.192833]\n",
      " [ 88.77286 ]\n",
      " [201.14275 ]\n",
      " [ 83.46237 ]\n",
      " [149.67775 ]\n",
      " [ 63.18019 ]\n",
      " [ 89.41395 ]\n",
      " [ 96.64514 ]\n",
      " [ 98.502335]\n",
      " [190.73795 ]\n",
      " [ 86.343414]\n",
      " [ 81.72454 ]\n",
      " [ 51.76864 ]\n",
      " [205.62892 ]\n",
      " [100.26852 ]\n",
      " [ 73.64284 ]\n",
      " [164.10577 ]\n",
      " [167.74986 ]\n",
      " [150.05836 ]\n",
      " [153.07834 ]\n",
      " [200.8982  ]\n",
      " [126.132904]\n",
      " [158.99821 ]\n",
      " [155.98242 ]\n",
      " [ 97.809296]\n",
      " [115.6344  ]\n",
      " [207.60696 ]\n",
      " [184.31514 ]\n",
      " [146.2588  ]\n",
      " [ 72.835945]\n",
      " [ 54.235703]\n",
      " [146.13368 ]\n",
      " [102.361206]\n",
      " [ 61.24321 ]\n",
      " [ 77.72345 ]\n",
      " [ 40.409065]\n",
      " [144.46736 ]\n",
      " [163.06099 ]\n",
      " [117.31261 ]\n",
      " [119.1162  ]\n",
      " [110.47359 ]\n",
      " [ 99.427734]\n",
      " [ 87.74823 ]\n",
      " [ 66.06503 ]\n",
      " [ 67.1335  ]\n",
      " [112.663246]\n",
      " [142.46416 ]\n",
      " [194.90282 ]\n",
      " [273.25876 ]\n",
      " [133.50923 ]\n",
      " [166.83186 ]\n",
      " [ 56.021355]\n",
      " [ 93.89284 ]\n",
      " [117.65712 ]\n",
      " [121.21559 ]\n",
      " [ 44.812607]\n",
      " [139.69858 ]\n",
      " [139.70454 ]\n",
      " [120.63719 ]\n",
      " [239.42046 ]\n",
      " [137.86212 ]\n",
      " [199.30328 ]\n",
      " [107.84093 ]\n",
      " [109.994354]\n",
      " [ 53.106667]\n",
      " [ 63.95514 ]\n",
      " [104.42962 ]\n",
      " [143.02989 ]\n",
      " [158.96027 ]\n",
      " [ 59.41745 ]\n",
      " [102.99195 ]\n",
      " [131.18268 ]\n",
      " [149.41646 ]\n",
      " [224.62796 ]\n",
      " [ 51.113686]\n",
      " [111.191536]\n",
      " [104.06233 ]\n",
      " [ 71.8609  ]\n",
      " [ 76.0004  ]\n",
      " [ 74.66813 ]\n",
      " [159.9899  ]\n",
      " [ 74.40901 ]\n",
      " [148.62076 ]\n",
      " [164.00932 ]\n",
      " [123.510956]\n",
      " [102.597   ]\n",
      " [118.64597 ]\n",
      " [104.94319 ]\n",
      " [161.05734 ]\n",
      " [182.63512 ]\n",
      " [214.82535 ]\n",
      " [160.74445 ]\n",
      " [ 44.9694  ]\n",
      " [231.7723  ]\n",
      " [ 85.2565  ]\n",
      " [140.40366 ]\n",
      " [147.05693 ]]\n",
      "=================\n",
      "R2 :  0.36545770953674594\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.5900\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.1532\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1088\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0648\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6302\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.9086\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4398\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7674\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9638\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.4774\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.9587\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.2377\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.7555\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8548\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.1338\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6404\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0746\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9897\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4182\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4701\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4413\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.5439\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0153\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4427\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2466\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.4271\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6048\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8490\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4283\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.9989\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3647\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.6361\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2831\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.2607\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.9362\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8188\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3898\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.0923\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4675\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4923\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6086\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4988\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4655\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6282\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0309\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.4126\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5599\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1146\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.3429\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.8243\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.2290\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.5531\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4836\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.1969\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3867\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0734\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0490\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.2110\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4513\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3933\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2299\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0584\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8905\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7899\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3566\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3365\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0205\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5377\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.7453\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7637\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5963\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9690\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0902\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.0409\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.9687\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6793\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1359\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.8666\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 202us/step - loss: 35.8666\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 46.7339\n",
      "loss :  46.73385238647461\n",
      "5/5 [==============================] - 0s 748us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[114.93719 ]\n",
      " [113.326744]\n",
      " [216.10173 ]\n",
      " [154.96974 ]\n",
      " [132.08397 ]\n",
      " [141.2425  ]\n",
      " [272.83658 ]\n",
      " [ 85.104   ]\n",
      " [ 79.77662 ]\n",
      " [183.26067 ]\n",
      " [165.80128 ]\n",
      " [147.6378  ]\n",
      " [117.71943 ]\n",
      " [190.40086 ]\n",
      " [250.62141 ]\n",
      " [149.02098 ]\n",
      " [100.501076]\n",
      " [189.5554  ]\n",
      " [144.14113 ]\n",
      " [166.1791  ]\n",
      " [165.75795 ]\n",
      " [221.01604 ]\n",
      " [115.04269 ]\n",
      " [ 47.850807]\n",
      " [ 75.10516 ]\n",
      " [181.90654 ]\n",
      " [103.274895]\n",
      " [107.60155 ]\n",
      " [137.45485 ]\n",
      " [143.71375 ]\n",
      " [ 94.66692 ]\n",
      " [238.4608  ]\n",
      " [187.90758 ]\n",
      " [214.87563 ]\n",
      " [163.24788 ]\n",
      " [114.60923 ]\n",
      " [ 49.4816  ]\n",
      " [105.19385 ]\n",
      " [222.2347  ]\n",
      " [ 93.61509 ]\n",
      " [164.29062 ]\n",
      " [ 72.83593 ]\n",
      " [110.91329 ]\n",
      " [111.59637 ]\n",
      " [117.66001 ]\n",
      " [206.96733 ]\n",
      " [ 97.456184]\n",
      " [111.85844 ]\n",
      " [ 61.637638]\n",
      " [221.48656 ]\n",
      " [118.82053 ]\n",
      " [ 84.17295 ]\n",
      " [183.69612 ]\n",
      " [181.1127  ]\n",
      " [174.79579 ]\n",
      " [160.328   ]\n",
      " [219.3359  ]\n",
      " [139.15404 ]\n",
      " [185.07845 ]\n",
      " [173.4182  ]\n",
      " [107.25389 ]\n",
      " [150.36235 ]\n",
      " [225.91008 ]\n",
      " [206.88774 ]\n",
      " [156.86392 ]\n",
      " [100.4323  ]\n",
      " [ 61.176968]\n",
      " [183.0196  ]\n",
      " [108.11943 ]\n",
      " [ 83.30908 ]\n",
      " [ 88.857506]\n",
      " [ 57.812237]\n",
      " [173.07611 ]\n",
      " [178.99017 ]\n",
      " [143.23451 ]\n",
      " [130.75264 ]\n",
      " [130.21208 ]\n",
      " [102.97176 ]\n",
      " [116.29857 ]\n",
      " [ 64.86504 ]\n",
      " [ 72.18495 ]\n",
      " [121.35495 ]\n",
      " [167.19865 ]\n",
      " [217.882   ]\n",
      " [295.54968 ]\n",
      " [149.51375 ]\n",
      " [185.09471 ]\n",
      " [ 60.97881 ]\n",
      " [118.6411  ]\n",
      " [148.11726 ]\n",
      " [135.92258 ]\n",
      " [ 58.22459 ]\n",
      " [160.9366  ]\n",
      " [160.18297 ]\n",
      " [148.11162 ]\n",
      " [259.7744  ]\n",
      " [169.76381 ]\n",
      " [218.02193 ]\n",
      " [124.67598 ]\n",
      " [123.34672 ]\n",
      " [ 63.369602]\n",
      " [ 76.03307 ]\n",
      " [125.08762 ]\n",
      " [172.98738 ]\n",
      " [171.92995 ]\n",
      " [ 76.25513 ]\n",
      " [116.17597 ]\n",
      " [153.38002 ]\n",
      " [167.64749 ]\n",
      " [240.84764 ]\n",
      " [ 60.35819 ]\n",
      " [128.94542 ]\n",
      " [130.64566 ]\n",
      " [ 97.26154 ]\n",
      " [ 75.325096]\n",
      " [104.43556 ]\n",
      " [179.31783 ]\n",
      " [ 90.038666]\n",
      " [172.01335 ]\n",
      " [192.8347  ]\n",
      " [137.10625 ]\n",
      " [111.81681 ]\n",
      " [140.89891 ]\n",
      " [131.63142 ]\n",
      " [177.6863  ]\n",
      " [201.32867 ]\n",
      " [240.07054 ]\n",
      " [181.46191 ]\n",
      " [ 55.290016]\n",
      " [251.23454 ]\n",
      " [ 99.910576]\n",
      " [177.74449 ]\n",
      " [194.21355 ]]\n",
      "=================\n",
      "R2 :  0.4081850564045427\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5258\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.1878\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5932\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.2504\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9300\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3788\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2593\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4165\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8773\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1973\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5427\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6232\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6240\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7198\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.8462\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.9952\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8002\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6044\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2029\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.2539\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.8184\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.8650\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7688\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6343\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4964\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7762\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.4371\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7368\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4598\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8621\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.4123\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9620\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6409\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.1997\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9468\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3797\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5038\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3414\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.0772\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6643\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2238\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.7284\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.0088\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.4778\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3744\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7974\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.1142\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5597\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5689\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7568\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.6623\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4801\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5725\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8529\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1183\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.9524\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4606\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.6781\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.4843\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3545\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3139\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.3616\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8737\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9813\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.1870\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9735\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8641\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8289\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.8156\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.2788\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7804\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0294\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.0686\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.0993\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3025\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4034\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4309\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.7920\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 35.7920\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 48.7161\n",
      "loss :  48.716121673583984\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[112.17161 ]\n",
      " [119.70786 ]\n",
      " [234.54579 ]\n",
      " [167.0224  ]\n",
      " [127.421036]\n",
      " [110.57194 ]\n",
      " [291.17493 ]\n",
      " [ 90.32457 ]\n",
      " [ 88.842285]\n",
      " [181.5343  ]\n",
      " [167.7904  ]\n",
      " [155.78996 ]\n",
      " [114.246284]\n",
      " [208.85687 ]\n",
      " [275.87512 ]\n",
      " [165.44305 ]\n",
      " [112.32444 ]\n",
      " [218.4012  ]\n",
      " [150.75981 ]\n",
      " [189.49747 ]\n",
      " [183.51251 ]\n",
      " [240.71121 ]\n",
      " [119.50648 ]\n",
      " [ 51.782673]\n",
      " [ 81.87555 ]\n",
      " [198.95464 ]\n",
      " [111.09558 ]\n",
      " [121.08124 ]\n",
      " [128.73459 ]\n",
      " [145.70969 ]\n",
      " [103.444984]\n",
      " [264.40076 ]\n",
      " [212.03058 ]\n",
      " [225.87265 ]\n",
      " [172.5173  ]\n",
      " [123.95578 ]\n",
      " [ 58.431637]\n",
      " [114.44946 ]\n",
      " [243.73486 ]\n",
      " [114.89327 ]\n",
      " [178.65874 ]\n",
      " [ 79.18055 ]\n",
      " [119.24172 ]\n",
      " [114.40854 ]\n",
      " [125.82177 ]\n",
      " [228.18976 ]\n",
      " [120.67713 ]\n",
      " [116.50892 ]\n",
      " [ 63.04153 ]\n",
      " [242.2633  ]\n",
      " [142.52917 ]\n",
      " [ 95.59561 ]\n",
      " [197.49922 ]\n",
      " [199.48648 ]\n",
      " [194.34283 ]\n",
      " [167.50679 ]\n",
      " [241.59369 ]\n",
      " [150.24878 ]\n",
      " [198.49767 ]\n",
      " [187.71082 ]\n",
      " [117.74616 ]\n",
      " [161.37329 ]\n",
      " [250.52007 ]\n",
      " [227.31725 ]\n",
      " [169.42136 ]\n",
      " [103.31668 ]\n",
      " [ 66.22202 ]\n",
      " [195.7417  ]\n",
      " [111.57084 ]\n",
      " [ 98.88581 ]\n",
      " [ 97.94169 ]\n",
      " [ 61.149982]\n",
      " [194.3158  ]\n",
      " [194.59805 ]\n",
      " [176.01083 ]\n",
      " [140.8676  ]\n",
      " [152.27094 ]\n",
      " [105.40543 ]\n",
      " [135.00404 ]\n",
      " [ 70.34906 ]\n",
      " [ 93.160225]\n",
      " [130.705   ]\n",
      " [188.52954 ]\n",
      " [242.9408  ]\n",
      " [328.12    ]\n",
      " [153.45697 ]\n",
      " [201.96225 ]\n",
      " [ 66.53672 ]\n",
      " [110.26481 ]\n",
      " [163.13507 ]\n",
      " [144.55182 ]\n",
      " [ 59.703243]\n",
      " [166.70883 ]\n",
      " [170.14857 ]\n",
      " [157.07672 ]\n",
      " [287.28912 ]\n",
      " [210.99301 ]\n",
      " [239.93558 ]\n",
      " [132.09314 ]\n",
      " [132.74683 ]\n",
      " [ 69.79484 ]\n",
      " [ 88.08354 ]\n",
      " [133.59517 ]\n",
      " [174.76672 ]\n",
      " [185.5595  ]\n",
      " [ 85.14852 ]\n",
      " [129.12843 ]\n",
      " [154.80026 ]\n",
      " [171.4241  ]\n",
      " [265.41635 ]\n",
      " [ 70.83112 ]\n",
      " [138.44797 ]\n",
      " [152.8011  ]\n",
      " [103.187454]\n",
      " [ 80.36248 ]\n",
      " [103.091225]\n",
      " [184.84795 ]\n",
      " [111.8619  ]\n",
      " [185.39447 ]\n",
      " [203.98799 ]\n",
      " [151.23996 ]\n",
      " [117.57313 ]\n",
      " [143.25345 ]\n",
      " [145.90656 ]\n",
      " [194.30634 ]\n",
      " [219.0026  ]\n",
      " [265.71423 ]\n",
      " [199.47818 ]\n",
      " [ 59.53119 ]\n",
      " [266.3492  ]\n",
      " [115.13264 ]\n",
      " [190.54337 ]\n",
      " [186.6264  ]]\n",
      "=================\n",
      "R2 :  0.37567140465481663\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.8401\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1774\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6327\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.5918\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4983\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6396\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.1657\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4561\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5001\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.6675\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8042\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.1749\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6006\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.5952\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0724\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9470\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.4724\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1404\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8424\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.4007\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2719\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6415\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.3850\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.2080\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2304\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5739\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5907\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6846\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.0830\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.6171\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.8351\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2489\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5676\\3301150971.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 4. 모델 컴파일\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "now = datetime.now()\n",
    "model.compile(loss=\"mae\",optimizer=\"adam\")\n",
    "f = open(\"C:\\study\\keras\\diabets.txt\",'a')\n",
    "\n",
    "# 4. 모델 컴파일\n",
    "while (True):\n",
    "    model.fit(x_train,y_train,epochs=100,batch_size=4,steps_per_epoch=100)\n",
    "    loss = model.evaluate(x_test,y_test)\n",
    "    print(\"loss : \",loss)\n",
    "    y_predict =model.predict(x_test)\n",
    "    print(\"=================\")\n",
    "    print(y_test)\n",
    "    print(y_predict)\n",
    "    print(\"=================\")\n",
    "    r2 = r2_score(y_test,y_predict)\n",
    "    print(\"R2 : \",r2)\n",
    "    \n",
    "    f.write(str(r2)+\"\\n\") \n",
    "    \n",
    "    if r2 >= 0.62 :\n",
    "        f.write(now+str(r2)+\"\\n\") \n",
    "        model.save(\"diabets.h5\")\n",
    "        f.cloes()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
