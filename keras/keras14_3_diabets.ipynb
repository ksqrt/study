{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "x = dataset.data\n",
    "y = dataset.target\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 사이킥 런의 칼럼 조회기능\n",
    "print(dataset.feature_names)\n",
    "print(len(dataset.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사이킥런의 데이터 요약\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델구성\n",
    "# 실습 train 0.7 이상\n",
    "# 평가지표 R2 : 0.8 이상으로 뽑아내보기 / RMSE 사용\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,\n",
    "    train_size=0.7,\n",
    "    shuffle = True,\n",
    "    random_state= 123\n",
    ")\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,881\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "number1 = randrange(1,10,1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10,input_dim = 10),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 155.0460\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 89.4763\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 65.6150\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 58.8141\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 51.4852\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 49.4258\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 48.0798\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 44.4739\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 46.4521\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.6138\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 45.6984\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9817\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.9140\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.1855\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9912\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.6215\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4037\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.7429\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.0780\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.4381\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.1971\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.5110\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8442\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.2976\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.4552\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.7289\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.6411\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5401\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.8211\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3666\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.7752\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.0261\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.7293\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0825\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.2112\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4460\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5591\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.2968\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.8461\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.9939\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.6263\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.4545\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.5551\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6268\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.7706\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.6607\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.5953\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.9474\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.9396\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8079\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.5390\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5224\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.6711\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.9247\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.7404\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.1043\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.0275\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.5727\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.7934\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.9722\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9584\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0790\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.6412\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.6187\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 41.5010\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2408\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.2612\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8794\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.1895\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.5284\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.4657\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6490\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4860\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0214\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9484\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.0293\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.4147\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.0083\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 39.0083\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 49.8082\n",
      "loss :  49.808162689208984\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[159.26714 ]\n",
      " [142.36673 ]\n",
      " [261.05035 ]\n",
      " [160.71983 ]\n",
      " [162.42691 ]\n",
      " [143.40948 ]\n",
      " [316.20227 ]\n",
      " [ 89.658066]\n",
      " [ 94.41282 ]\n",
      " [141.22903 ]\n",
      " [117.12384 ]\n",
      " [188.51126 ]\n",
      " [153.05148 ]\n",
      " [258.45764 ]\n",
      " [262.33368 ]\n",
      " [192.05925 ]\n",
      " [ 89.56702 ]\n",
      " [171.98872 ]\n",
      " [221.39445 ]\n",
      " [207.78836 ]\n",
      " [164.7403  ]\n",
      " [301.6677  ]\n",
      " [162.34627 ]\n",
      " [ 76.10725 ]\n",
      " [109.01716 ]\n",
      " [254.23366 ]\n",
      " [ 95.455956]\n",
      " [102.20058 ]\n",
      " [164.26884 ]\n",
      " [204.49986 ]\n",
      " [ 76.52915 ]\n",
      " [288.9745  ]\n",
      " [220.86455 ]\n",
      " [278.0033  ]\n",
      " [218.68849 ]\n",
      " [107.94123 ]\n",
      " [ 85.58357 ]\n",
      " [126.55264 ]\n",
      " [260.31052 ]\n",
      " [ 99.25918 ]\n",
      " [243.11981 ]\n",
      " [ 94.32137 ]\n",
      " [136.71092 ]\n",
      " [130.04558 ]\n",
      " [115.16507 ]\n",
      " [282.0589  ]\n",
      " [102.39655 ]\n",
      " [ 76.147125]\n",
      " [ 83.346825]\n",
      " [301.828   ]\n",
      " [ 91.25597 ]\n",
      " [ 90.07973 ]\n",
      " [208.50484 ]\n",
      " [251.9001  ]\n",
      " [152.96991 ]\n",
      " [249.3305  ]\n",
      " [295.71124 ]\n",
      " [185.90726 ]\n",
      " [205.93105 ]\n",
      " [194.26277 ]\n",
      " [123.351425]\n",
      " [130.56244 ]\n",
      " [300.37402 ]\n",
      " [246.09546 ]\n",
      " [240.8668  ]\n",
      " [ 75.15679 ]\n",
      " [ 84.66355 ]\n",
      " [163.44609 ]\n",
      " [135.08893 ]\n",
      " [ 97.12629 ]\n",
      " [109.479355]\n",
      " [ 77.37668 ]\n",
      " [140.23053 ]\n",
      " [212.64645 ]\n",
      " [139.7629  ]\n",
      " [174.73056 ]\n",
      " [140.96877 ]\n",
      " [152.7712  ]\n",
      " [129.1382  ]\n",
      " [ 98.32417 ]\n",
      " [ 83.84924 ]\n",
      " [168.45284 ]\n",
      " [122.44621 ]\n",
      " [269.20773 ]\n",
      " [302.72916 ]\n",
      " [176.47375 ]\n",
      " [258.07092 ]\n",
      " [ 96.01738 ]\n",
      " [135.06898 ]\n",
      " [122.11255 ]\n",
      " [171.78644 ]\n",
      " [ 78.92887 ]\n",
      " [184.58737 ]\n",
      " [179.76135 ]\n",
      " [ 96.35154 ]\n",
      " [328.3458  ]\n",
      " [148.00548 ]\n",
      " [283.16248 ]\n",
      " [150.66582 ]\n",
      " [140.55159 ]\n",
      " [ 87.336716]\n",
      " [ 77.82401 ]\n",
      " [137.53053 ]\n",
      " [200.76364 ]\n",
      " [239.77492 ]\n",
      " [ 93.88859 ]\n",
      " [128.31122 ]\n",
      " [152.84904 ]\n",
      " [212.02861 ]\n",
      " [310.78372 ]\n",
      " [ 92.420654]\n",
      " [122.19424 ]\n",
      " [126.45496 ]\n",
      " [ 75.23023 ]\n",
      " [ 91.27596 ]\n",
      " [ 76.80889 ]\n",
      " [227.2125  ]\n",
      " [ 86.329346]\n",
      " [191.00821 ]\n",
      " [215.30977 ]\n",
      " [140.2852  ]\n",
      " [155.05023 ]\n",
      " [149.48431 ]\n",
      " [140.99596 ]\n",
      " [246.80502 ]\n",
      " [284.3961  ]\n",
      " [311.2475  ]\n",
      " [201.83104 ]\n",
      " [ 78.17952 ]\n",
      " [261.88947 ]\n",
      " [ 94.00657 ]\n",
      " [155.66751 ]\n",
      " [141.38377 ]]\n",
      "=================\n",
      "R2 :  0.36079386258503887\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0568\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7390\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.9714\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 40.2932\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3105\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.8901\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.2032\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0705\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2432\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0853\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7154\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5834\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4274\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.1878\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3726\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3248\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7976\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 39.8453\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6484\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4753\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8771\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4074\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9185\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0964\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.5000\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1211\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0605\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6335\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.9954\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6058\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.3466\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3532\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6957\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7861\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2019\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6409\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5445\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5007\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6573\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8011\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8716\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8487\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2038\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3007\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.6516\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0128\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5167\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2448\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9694\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.4558\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7373\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5384\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8845\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8059\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1928\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1990\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7138\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8380\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7363\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2511\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.9948\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3629\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5914\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4488\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.1614\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.1066\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2069\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5990\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9077\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0778\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5114\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5056\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8694\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0081\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.8595\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6670\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9648\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2074\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 100us/step - loss: 36.2074\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 49.3405\n",
      "loss :  49.340518951416016\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[151.21774 ]\n",
      " [105.523766]\n",
      " [243.67574 ]\n",
      " [136.4098  ]\n",
      " [187.51158 ]\n",
      " [140.57245 ]\n",
      " [318.00433 ]\n",
      " [ 93.77599 ]\n",
      " [106.95037 ]\n",
      " [135.13246 ]\n",
      " [114.22258 ]\n",
      " [168.39787 ]\n",
      " [138.63574 ]\n",
      " [266.1057  ]\n",
      " [264.50693 ]\n",
      " [191.01132 ]\n",
      " [ 84.993195]\n",
      " [168.16559 ]\n",
      " [205.93132 ]\n",
      " [190.952   ]\n",
      " [140.65465 ]\n",
      " [315.87796 ]\n",
      " [152.89017 ]\n",
      " [ 75.64168 ]\n",
      " [ 92.43295 ]\n",
      " [266.31436 ]\n",
      " [ 91.15981 ]\n",
      " [ 93.96629 ]\n",
      " [168.11609 ]\n",
      " [213.95483 ]\n",
      " [ 83.88277 ]\n",
      " [281.24167 ]\n",
      " [223.21147 ]\n",
      " [292.29218 ]\n",
      " [219.70844 ]\n",
      " [ 96.88159 ]\n",
      " [ 86.572014]\n",
      " [129.50632 ]\n",
      " [186.38004 ]\n",
      " [ 98.21444 ]\n",
      " [269.03714 ]\n",
      " [ 82.29853 ]\n",
      " [132.17921 ]\n",
      " [109.51633 ]\n",
      " [101.15311 ]\n",
      " [284.59875 ]\n",
      " [134.1569  ]\n",
      " [ 78.84036 ]\n",
      " [ 78.40337 ]\n",
      " [299.26337 ]\n",
      " [ 82.515175]\n",
      " [ 86.28658 ]\n",
      " [213.83296 ]\n",
      " [226.81451 ]\n",
      " [139.43788 ]\n",
      " [238.23042 ]\n",
      " [286.10413 ]\n",
      " [195.01607 ]\n",
      " [226.97594 ]\n",
      " [196.08318 ]\n",
      " [113.97784 ]\n",
      " [119.12194 ]\n",
      " [290.5093  ]\n",
      " [204.12085 ]\n",
      " [232.56857 ]\n",
      " [ 75.16687 ]\n",
      " [ 79.061035]\n",
      " [147.40227 ]\n",
      " [114.04294 ]\n",
      " [ 93.406235]\n",
      " [ 91.98404 ]\n",
      " [ 78.44829 ]\n",
      " [161.42125 ]\n",
      " [226.53152 ]\n",
      " [138.24672 ]\n",
      " [160.33856 ]\n",
      " [150.21129 ]\n",
      " [142.34196 ]\n",
      " [127.38877 ]\n",
      " [ 90.60248 ]\n",
      " [ 79.51079 ]\n",
      " [145.15619 ]\n",
      " [104.69263 ]\n",
      " [266.1255  ]\n",
      " [287.86798 ]\n",
      " [171.23044 ]\n",
      " [269.78354 ]\n",
      " [ 86.8373  ]\n",
      " [128.5565  ]\n",
      " [113.95231 ]\n",
      " [159.203   ]\n",
      " [ 76.339836]\n",
      " [209.93163 ]\n",
      " [167.30383 ]\n",
      " [111.4632  ]\n",
      " [331.9331  ]\n",
      " [162.63121 ]\n",
      " [240.55249 ]\n",
      " [135.40941 ]\n",
      " [155.2274  ]\n",
      " [ 87.15958 ]\n",
      " [ 78.664764]\n",
      " [127.99004 ]\n",
      " [194.57973 ]\n",
      " [201.16388 ]\n",
      " [ 83.095085]\n",
      " [104.165665]\n",
      " [128.81216 ]\n",
      " [236.3369  ]\n",
      " [291.71936 ]\n",
      " [ 92.01246 ]\n",
      " [108.460754]\n",
      " [114.5649  ]\n",
      " [ 76.2043  ]\n",
      " [ 84.00121 ]\n",
      " [ 76.35705 ]\n",
      " [242.43521 ]\n",
      " [ 81.314865]\n",
      " [204.73782 ]\n",
      " [227.75253 ]\n",
      " [158.16466 ]\n",
      " [142.73154 ]\n",
      " [124.51027 ]\n",
      " [128.49374 ]\n",
      " [211.92003 ]\n",
      " [293.95346 ]\n",
      " [311.89178 ]\n",
      " [202.68895 ]\n",
      " [ 77.812325]\n",
      " [218.4594  ]\n",
      " [ 83.02708 ]\n",
      " [144.14352 ]\n",
      " [139.99193 ]]\n",
      "=================\n",
      "R2 :  0.3635034501673283\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4334\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8987\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3899\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4758\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6972\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4852\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6811\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5756\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0984\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.8281\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8550\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9005\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0949\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.9885\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.7169\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.0310\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2420\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.9813\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4048\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.8996\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4593\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.0707\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4476\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1638\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2342\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3789\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.5260\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4199\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.1457\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.1733\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5701\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8727\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5662\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 32.6768\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7251\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5000\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6296\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.6143\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2469\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 33.9062\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5578\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2525\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4127\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2372\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2629\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2944\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4260\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2027\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.7316\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6695\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.8958\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9698\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.5803\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1381\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8156\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.0425\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0803\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.4581\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8948\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.8968\n",
      "Epoch 61/100\n",
      " 75/100 [=====================>........] - ETA: 0s - loss: 34.7321"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5676\\948838227.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 4. 모델 컴파일\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1553\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_load_initial_step_from_ckpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m                     )\n\u001b[1;32m-> 1555\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1556\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[0;32m   1557\u001b[0m                             \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m             \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m             can_run_full_execution = (\n\u001b[0;32m   1376\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \"\"\"\n\u001b[0;32m   1156\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "now = datetime.now()\n",
    "model.compile(loss=\"mae\",optimizer=\"adam\")\n",
    "f = open(\"C:\\study\\keras\\diabets.txt\",'a')\n",
    "\n",
    "# 4. 모델 컴파일\n",
    "while (True):\n",
    "    model.fit(x_train,y_train,epochs=100,batch_size=4,steps_per_epoch=100)\n",
    "    loss = model.evaluate(x_test,y_test)\n",
    "    print(\"loss : \",loss)\n",
    "    y_predict =model.predict(x_test)\n",
    "    print(\"=================\")\n",
    "    print(y_test)\n",
    "    print(y_predict)\n",
    "    print(\"=================\")\n",
    "    r2 = r2_score(y_test,y_predict)\n",
    "    print(\"R2 : \",r2)\n",
    "    \n",
    "    f.write(str(now)+str(r2)+\"\\n\") \n",
    "    \n",
    "    if r2 >= 0.62 :\n",
    "        f.write(str(now)+str(r2)+\"\\n\") \n",
    "        model.save(\"diabets.h5\")\n",
    "        f.cloes()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
