{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "x = dataset.data\n",
    "y = dataset.target\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 사이킥 런의 칼럼 조회기능\n",
    "print(dataset.feature_names)\n",
    "print(len(dataset.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사이킥런의 데이터 요약\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델구성\n",
    "# 실습 train 0.7 이상\n",
    "# 평가지표 R2 : 0.8 이상으로 뽑아내보기 / RMSE 사용\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,\n",
    "    train_size=0.7,\n",
    "    shuffle = True,\n",
    "    random_state= 123\n",
    ")\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,881\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "number1 = randrange(1,10,1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10,input_dim = 10),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 126.7741\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 63.4174\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 54.4221\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 48.2679\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 47.5473\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.9087\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8657\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8263\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 44.8866\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.9239\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.6839\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.8162\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1236\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 45.0751\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.4579\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 45.4188\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2889\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.8638\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 45.3237\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.7769\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9260\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.6523\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.4976\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.1402\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 45.6931\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4547\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.8190\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.9302\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.1116\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1579\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2286\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8126\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8765\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.7917\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2910\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.7905\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.3804\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.6112\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5145\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.9720\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.8921\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3713\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 43.6263\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4783\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.6665\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.0372\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 44.0046\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4370\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8277\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.6947\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.4433\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1012\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.9526\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5834\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 42.4611\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.8004\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.7715\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.0983\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4807\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.4525\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1846\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.6287\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.5074\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8957\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3184\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.9502\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 43.2263\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.1952\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.4241\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.5442\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 42.1008\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9386\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2841\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3597\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.2490\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0219\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7230\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.3913\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 41.3913\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 43.2842\n",
      "loss :  43.284202575683594\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[148.34561 ]\n",
      " [128.78954 ]\n",
      " [230.4359  ]\n",
      " [170.27579 ]\n",
      " [157.44612 ]\n",
      " [156.46367 ]\n",
      " [273.40894 ]\n",
      " [ 95.37404 ]\n",
      " [ 98.65554 ]\n",
      " [120.36693 ]\n",
      " [113.08013 ]\n",
      " [180.23808 ]\n",
      " [140.42157 ]\n",
      " [212.52974 ]\n",
      " [240.90295 ]\n",
      " [165.49506 ]\n",
      " [103.12888 ]\n",
      " [154.75835 ]\n",
      " [187.80571 ]\n",
      " [175.08821 ]\n",
      " [175.8177  ]\n",
      " [246.20247 ]\n",
      " [117.915474]\n",
      " [ 73.4231  ]\n",
      " [ 97.439735]\n",
      " [198.92068 ]\n",
      " [ 96.7121  ]\n",
      " [101.75334 ]\n",
      " [150.46701 ]\n",
      " [170.03876 ]\n",
      " [ 87.157486]\n",
      " [245.58087 ]\n",
      " [209.3074  ]\n",
      " [230.1083  ]\n",
      " [188.73303 ]\n",
      " [ 92.0633  ]\n",
      " [ 78.08786 ]\n",
      " [114.45054 ]\n",
      " [240.56772 ]\n",
      " [102.09799 ]\n",
      " [196.78574 ]\n",
      " [ 83.107254]\n",
      " [117.443436]\n",
      " [132.96458 ]\n",
      " [100.53217 ]\n",
      " [235.3601  ]\n",
      " [ 98.86368 ]\n",
      " [ 87.81526 ]\n",
      " [ 82.31503 ]\n",
      " [236.09299 ]\n",
      " [100.96775 ]\n",
      " [ 96.10249 ]\n",
      " [162.0546  ]\n",
      " [182.52257 ]\n",
      " [157.89442 ]\n",
      " [194.51309 ]\n",
      " [227.57558 ]\n",
      " [151.59831 ]\n",
      " [193.84543 ]\n",
      " [186.93143 ]\n",
      " [118.92247 ]\n",
      " [109.982574]\n",
      " [233.83163 ]\n",
      " [207.67314 ]\n",
      " [194.29901 ]\n",
      " [ 72.762634]\n",
      " [ 79.307106]\n",
      " [163.84482 ]\n",
      " [102.767105]\n",
      " [ 87.55589 ]\n",
      " [107.57353 ]\n",
      " [ 69.434555]\n",
      " [156.14365 ]\n",
      " [203.65067 ]\n",
      " [129.38335 ]\n",
      " [162.47224 ]\n",
      " [141.0249  ]\n",
      " [120.0927  ]\n",
      " [ 98.61512 ]\n",
      " [ 86.37293 ]\n",
      " [ 78.23732 ]\n",
      " [131.30391 ]\n",
      " [133.65492 ]\n",
      " [226.09631 ]\n",
      " [291.6783  ]\n",
      " [164.7172  ]\n",
      " [211.51222 ]\n",
      " [ 79.53402 ]\n",
      " [118.73677 ]\n",
      " [127.690056]\n",
      " [161.20633 ]\n",
      " [ 74.181   ]\n",
      " [173.86238 ]\n",
      " [147.76746 ]\n",
      " [111.57868 ]\n",
      " [277.68164 ]\n",
      " [150.54207 ]\n",
      " [227.91675 ]\n",
      " [128.89975 ]\n",
      " [137.45189 ]\n",
      " [ 79.7572  ]\n",
      " [ 74.647705]\n",
      " [117.93588 ]\n",
      " [167.38661 ]\n",
      " [186.79561 ]\n",
      " [ 92.65464 ]\n",
      " [126.84549 ]\n",
      " [162.50185 ]\n",
      " [193.56496 ]\n",
      " [249.75836 ]\n",
      " [ 89.79404 ]\n",
      " [124.977585]\n",
      " [115.42583 ]\n",
      " [ 71.4352  ]\n",
      " [ 92.96425 ]\n",
      " [ 78.635735]\n",
      " [197.81108 ]\n",
      " [ 81.58667 ]\n",
      " [193.24266 ]\n",
      " [196.65236 ]\n",
      " [137.26047 ]\n",
      " [139.48697 ]\n",
      " [141.05348 ]\n",
      " [114.62523 ]\n",
      " [190.79202 ]\n",
      " [221.56966 ]\n",
      " [249.35081 ]\n",
      " [196.94267 ]\n",
      " [ 72.00682 ]\n",
      " [246.37057 ]\n",
      " [ 98.529816]\n",
      " [148.43379 ]\n",
      " [141.07632 ]]\n",
      "=================\n",
      "R2 :  0.5095816557772879\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.6587\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.9146\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3863\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.8172\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8264\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.4996\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.3213\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.4695\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2663\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.1658\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6540\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6705\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.9386\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.7434\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.4053\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.6079\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.5465\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6538\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.1792\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6332\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.8050\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.2509\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.6813\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2019\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.7307\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3624\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3684\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.6189\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.3778\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.6411\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 40.0167\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.5281\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3413\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 41.0485\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.0878\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.7700\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0145\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4914\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.9300\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.1261\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2767\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2727\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.6449\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.4976\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.8805\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.2446\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.3407\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4743\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.0819\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.9434\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.9415\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.4019\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.7822\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 39.1068\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6378\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.0347\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.3549\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.8684\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.0652\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1831\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.2576\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.8536\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.1443\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.2026\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.7995\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 38.0771\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5993\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.7420\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.6180\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 38.4610\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.3042\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.8638\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 34.9196\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.4528\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.9379\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 36.3139\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.4796\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 37.1519\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 140us/step - loss: 37.1519\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 45.4752\n",
      "loss :  45.47521209716797\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[148.99239 ]\n",
      " [127.78208 ]\n",
      " [219.67216 ]\n",
      " [165.47021 ]\n",
      " [157.60143 ]\n",
      " [183.30103 ]\n",
      " [280.03818 ]\n",
      " [ 71.88117 ]\n",
      " [124.256546]\n",
      " [152.56137 ]\n",
      " [136.40572 ]\n",
      " [176.3521  ]\n",
      " [123.35678 ]\n",
      " [215.192   ]\n",
      " [245.50966 ]\n",
      " [173.86256 ]\n",
      " [ 83.92634 ]\n",
      " [158.26775 ]\n",
      " [182.59999 ]\n",
      " [179.07132 ]\n",
      " [177.4705  ]\n",
      " [241.4031  ]\n",
      " [127.90834 ]\n",
      " [ 71.31451 ]\n",
      " [ 71.670006]\n",
      " [191.46487 ]\n",
      " [ 98.97856 ]\n",
      " [121.24264 ]\n",
      " [141.4046  ]\n",
      " [165.85509 ]\n",
      " [104.36411 ]\n",
      " [232.29657 ]\n",
      " [209.1137  ]\n",
      " [276.5714  ]\n",
      " [183.5567  ]\n",
      " [106.70874 ]\n",
      " [ 70.90193 ]\n",
      " [128.8316  ]\n",
      " [235.28711 ]\n",
      " [108.18329 ]\n",
      " [194.75381 ]\n",
      " [ 80.55659 ]\n",
      " [122.57865 ]\n",
      " [130.8819  ]\n",
      " [ 99.1252  ]\n",
      " [234.04434 ]\n",
      " [ 89.78468 ]\n",
      " [101.73533 ]\n",
      " [ 70.90193 ]\n",
      " [238.72404 ]\n",
      " [107.79085 ]\n",
      " [117.592384]\n",
      " [186.22298 ]\n",
      " [188.245   ]\n",
      " [172.0292  ]\n",
      " [178.86949 ]\n",
      " [239.88068 ]\n",
      " [151.72292 ]\n",
      " [201.3131  ]\n",
      " [182.89827 ]\n",
      " [132.97095 ]\n",
      " [120.65788 ]\n",
      " [255.37744 ]\n",
      " [210.25127 ]\n",
      " [189.94818 ]\n",
      " [ 73.16207 ]\n",
      " [ 76.543396]\n",
      " [158.00905 ]\n",
      " [132.79088 ]\n",
      " [ 79.573166]\n",
      " [105.13235 ]\n",
      " [ 70.90193 ]\n",
      " [179.75882 ]\n",
      " [206.5792  ]\n",
      " [123.13521 ]\n",
      " [142.65663 ]\n",
      " [144.77608 ]\n",
      " [127.63464 ]\n",
      " [101.10452 ]\n",
      " [ 84.878494]\n",
      " [ 77.01979 ]\n",
      " [135.22641 ]\n",
      " [152.85088 ]\n",
      " [220.29823 ]\n",
      " [239.41125 ]\n",
      " [172.33534 ]\n",
      " [211.68193 ]\n",
      " [ 78.70817 ]\n",
      " [110.31704 ]\n",
      " [132.26251 ]\n",
      " [147.56377 ]\n",
      " [ 79.705605]\n",
      " [183.16812 ]\n",
      " [168.78906 ]\n",
      " [140.54828 ]\n",
      " [285.38336 ]\n",
      " [175.34512 ]\n",
      " [221.61185 ]\n",
      " [133.64423 ]\n",
      " [151.32487 ]\n",
      " [ 78.97158 ]\n",
      " [ 74.33071 ]\n",
      " [123.83696 ]\n",
      " [188.20493 ]\n",
      " [177.14035 ]\n",
      " [ 70.92296 ]\n",
      " [137.7287  ]\n",
      " [156.44035 ]\n",
      " [207.74095 ]\n",
      " [262.26825 ]\n",
      " [ 86.19813 ]\n",
      " [140.53961 ]\n",
      " [ 96.18222 ]\n",
      " [ 78.00879 ]\n",
      " [ 73.02934 ]\n",
      " [ 89.61273 ]\n",
      " [199.74564 ]\n",
      " [ 70.90193 ]\n",
      " [231.62082 ]\n",
      " [243.4986  ]\n",
      " [146.6132  ]\n",
      " [129.0869  ]\n",
      " [133.30328 ]\n",
      " [123.90781 ]\n",
      " [186.38855 ]\n",
      " [204.20508 ]\n",
      " [261.20392 ]\n",
      " [199.6529  ]\n",
      " [ 70.901924]\n",
      " [176.61127 ]\n",
      " [ 85.61816 ]\n",
      " [175.59393 ]\n",
      " [186.83731 ]]\n",
      "=================\n",
      "R2 :  0.4653501069079312\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1626\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5817\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3344\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.5414\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.1970\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 37.2588\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2588\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3068\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.8832\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1439\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4014\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6367\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.3309\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6774\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.2609\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9678\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9971\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5887\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5006\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.6268\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.8288\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.2941\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9767\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.4786\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2005\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 35.1721\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.0511\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2598\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3254\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.3545\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.5263\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9583\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.4462\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.9081\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.2510\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6309\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9182\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.6179\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2329\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6359\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.7239\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 31.9250\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5731\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.8757\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.1492\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.8188\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9221\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.5631\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.8476\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.3881\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.8538\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.9223\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.0816\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.2812\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.0803\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.0784\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 31.5883\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.6347\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 36.6243\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.6144\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.8823\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.0497\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9800\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.2594\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.2910\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6768\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.4993\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.1137\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.1959\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.0895\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.5502\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.2591\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.1262\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.0627\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.0086\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 35.4853\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.2961\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.5832\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 32.5832\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 46.8284\n",
      "loss :  46.82844161987305\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[166.15948 ]\n",
      " [127.99838 ]\n",
      " [226.1454  ]\n",
      " [159.67688 ]\n",
      " [141.94554 ]\n",
      " [202.69945 ]\n",
      " [294.6641  ]\n",
      " [ 70.42836 ]\n",
      " [136.31087 ]\n",
      " [156.71925 ]\n",
      " [156.98537 ]\n",
      " [195.61414 ]\n",
      " [130.677   ]\n",
      " [242.18854 ]\n",
      " [279.09558 ]\n",
      " [185.91002 ]\n",
      " [ 69.716965]\n",
      " [167.16219 ]\n",
      " [185.06146 ]\n",
      " [193.08342 ]\n",
      " [187.74818 ]\n",
      " [281.24362 ]\n",
      " [135.31915 ]\n",
      " [ 69.716965]\n",
      " [ 69.716965]\n",
      " [198.79732 ]\n",
      " [107.24383 ]\n",
      " [120.67283 ]\n",
      " [142.57782 ]\n",
      " [171.15453 ]\n",
      " [152.19159 ]\n",
      " [254.41855 ]\n",
      " [222.34071 ]\n",
      " [314.41147 ]\n",
      " [202.15474 ]\n",
      " [108.794685]\n",
      " [ 69.716965]\n",
      " [117.93433 ]\n",
      " [279.66492 ]\n",
      " [117.3119  ]\n",
      " [200.42818 ]\n",
      " [ 68.41055 ]\n",
      " [118.36597 ]\n",
      " [101.39117 ]\n",
      " [ 95.41415 ]\n",
      " [284.92764 ]\n",
      " [ 79.47894 ]\n",
      " [113.696335]\n",
      " [ 69.716965]\n",
      " [271.40604 ]\n",
      " [139.30159 ]\n",
      " [111.01516 ]\n",
      " [186.21214 ]\n",
      " [214.70084 ]\n",
      " [198.69226 ]\n",
      " [194.48497 ]\n",
      " [245.3806  ]\n",
      " [170.06456 ]\n",
      " [219.89996 ]\n",
      " [179.49956 ]\n",
      " [119.76601 ]\n",
      " [109.22309 ]\n",
      " [275.39612 ]\n",
      " [214.08391 ]\n",
      " [220.98672 ]\n",
      " [ 69.716965]\n",
      " [ 55.770756]\n",
      " [135.84615 ]\n",
      " [131.72139 ]\n",
      " [ 79.63287 ]\n",
      " [ 99.811295]\n",
      " [ 69.716965]\n",
      " [193.65236 ]\n",
      " [221.53636 ]\n",
      " [103.797485]\n",
      " [146.4871  ]\n",
      " [131.87383 ]\n",
      " [129.3362  ]\n",
      " [106.91064 ]\n",
      " [ 78.911575]\n",
      " [ 69.716965]\n",
      " [148.66264 ]\n",
      " [165.79257 ]\n",
      " [252.56345 ]\n",
      " [253.72185 ]\n",
      " [179.80565 ]\n",
      " [232.73862 ]\n",
      " [ 60.57066 ]\n",
      " [111.90043 ]\n",
      " [135.90445 ]\n",
      " [149.1213  ]\n",
      " [ 90.83531 ]\n",
      " [195.37541 ]\n",
      " [172.80371 ]\n",
      " [135.6374  ]\n",
      " [313.7491  ]\n",
      " [174.25157 ]\n",
      " [238.77821 ]\n",
      " [122.10025 ]\n",
      " [148.87985 ]\n",
      " [ 89.52835 ]\n",
      " [ 96.485886]\n",
      " [126.59738 ]\n",
      " [213.5946  ]\n",
      " [179.09067 ]\n",
      " [ 69.716965]\n",
      " [128.69887 ]\n",
      " [147.65565 ]\n",
      " [204.94325 ]\n",
      " [295.07742 ]\n",
      " [109.6108  ]\n",
      " [142.34515 ]\n",
      " [ 83.90716 ]\n",
      " [ 76.55031 ]\n",
      " [ 79.885765]\n",
      " [ 96.42905 ]\n",
      " [224.27164 ]\n",
      " [ 69.716965]\n",
      " [246.9309  ]\n",
      " [260.52103 ]\n",
      " [134.19038 ]\n",
      " [130.28891 ]\n",
      " [132.45578 ]\n",
      " [133.34073 ]\n",
      " [205.63005 ]\n",
      " [214.83328 ]\n",
      " [288.7964  ]\n",
      " [205.072   ]\n",
      " [ 69.716965]\n",
      " [193.64026 ]\n",
      " [ 80.4754  ]\n",
      " [198.34967 ]\n",
      " [151.72035 ]]\n",
      "=================\n",
      "R2 :  0.4272088689213913\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.7356\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 34.1341\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.7402\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.6983\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.4779\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.5814\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.5133\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.2388\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.7577\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 30.9288\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.2065\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.0794\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.4557\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.3949\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.0046\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.1345\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.8293\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.9400\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.7990\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.9656\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.7377\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.4704\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.3970\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.7539\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 33.2315\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.1662\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.5444\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.2285\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.3858\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.8736\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.0529\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.1689\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.7093\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.9849\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.7766\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.1441\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.6063\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.7926\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.6192\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.7438\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.8360\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.8880\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.9000\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.5581\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.1137\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.9204\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.2024\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.1981\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.3639\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.0837\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.6718\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.3915\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.2943\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.6562\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.1551\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.4365\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.7088\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.3274\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.0306\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.0539\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.7550\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.0699\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 32.1349\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.7150\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.2500\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.5312\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 31.1388\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.1843\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.2255\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.4557\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.8358\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.6455\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.6592\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.3748\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.3851\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.4765\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.8884\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.9200\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 27.9200\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 46.5077\n",
      "loss :  46.50771713256836\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[171.80324 ]\n",
      " [ 69.20866 ]\n",
      " [212.49757 ]\n",
      " [151.69562 ]\n",
      " [151.90309 ]\n",
      " [212.04256 ]\n",
      " [291.94446 ]\n",
      " [ 67.867805]\n",
      " [137.58838 ]\n",
      " [159.76338 ]\n",
      " [173.93018 ]\n",
      " [175.40517 ]\n",
      " [115.10439 ]\n",
      " [244.14133 ]\n",
      " [261.79312 ]\n",
      " [174.9696  ]\n",
      " [ 68.85648 ]\n",
      " [151.63608 ]\n",
      " [183.0939  ]\n",
      " [210.76004 ]\n",
      " [200.36717 ]\n",
      " [282.17697 ]\n",
      " [127.040085]\n",
      " [ 67.15066 ]\n",
      " [ 68.85648 ]\n",
      " [184.18658 ]\n",
      " [ 97.38281 ]\n",
      " [121.09824 ]\n",
      " [144.08896 ]\n",
      " [165.77118 ]\n",
      " [129.55779 ]\n",
      " [239.49179 ]\n",
      " [192.96034 ]\n",
      " [282.37143 ]\n",
      " [193.47644 ]\n",
      " [100.348625]\n",
      " [ 58.956448]\n",
      " [123.61098 ]\n",
      " [297.2232  ]\n",
      " [110.27888 ]\n",
      " [237.30685 ]\n",
      " [ 68.85648 ]\n",
      " [130.15598 ]\n",
      " [ 72.89758 ]\n",
      " [ 86.500854]\n",
      " [294.99203 ]\n",
      " [ 68.85648 ]\n",
      " [ 84.42445 ]\n",
      " [ 60.674778]\n",
      " [278.45633 ]\n",
      " [144.84673 ]\n",
      " [106.954216]\n",
      " [192.3635  ]\n",
      " [207.31392 ]\n",
      " [219.31659 ]\n",
      " [233.0714  ]\n",
      " [237.02269 ]\n",
      " [156.3217  ]\n",
      " [219.37627 ]\n",
      " [165.5204  ]\n",
      " [113.593704]\n",
      " [107.25076 ]\n",
      " [256.74408 ]\n",
      " [208.0658  ]\n",
      " [213.23143 ]\n",
      " [ 64.53526 ]\n",
      " [ 50.993324]\n",
      " [117.268394]\n",
      " [109.927284]\n",
      " [ 68.85648 ]\n",
      " [ 72.70768 ]\n",
      " [ 68.85648 ]\n",
      " [193.78105 ]\n",
      " [217.93013 ]\n",
      " [105.52303 ]\n",
      " [ 78.69489 ]\n",
      " [122.42063 ]\n",
      " [128.49893 ]\n",
      " [ 84.56459 ]\n",
      " [ 68.85648 ]\n",
      " [ 68.85648 ]\n",
      " [124.96673 ]\n",
      " [169.4555  ]\n",
      " [246.00928 ]\n",
      " [239.37196 ]\n",
      " [170.64624 ]\n",
      " [244.72511 ]\n",
      " [ 68.85648 ]\n",
      " [114.813286]\n",
      " [121.89987 ]\n",
      " [136.08379 ]\n",
      " [ 76.74751 ]\n",
      " [197.23607 ]\n",
      " [184.05214 ]\n",
      " [125.605156]\n",
      " [283.7894  ]\n",
      " [145.67903 ]\n",
      " [240.30956 ]\n",
      " [118.413345]\n",
      " [149.85728 ]\n",
      " [ 73.39522 ]\n",
      " [ 83.566765]\n",
      " [123.538475]\n",
      " [217.62282 ]\n",
      " [179.15096 ]\n",
      " [ 66.131996]\n",
      " [122.322136]\n",
      " [137.52002 ]\n",
      " [199.03648 ]\n",
      " [288.20953 ]\n",
      " [ 76.712585]\n",
      " [140.78108 ]\n",
      " [ 68.28636 ]\n",
      " [ 68.85648 ]\n",
      " [ 60.090355]\n",
      " [ 85.50423 ]\n",
      " [218.88268 ]\n",
      " [ 68.85648 ]\n",
      " [238.58286 ]\n",
      " [229.0385  ]\n",
      " [132.87593 ]\n",
      " [124.655266]\n",
      " [107.257   ]\n",
      " [171.82266 ]\n",
      " [186.94888 ]\n",
      " [194.08012 ]\n",
      " [296.8576  ]\n",
      " [214.1887  ]\n",
      " [ 63.066338]\n",
      " [160.03784 ]\n",
      " [ 68.85648 ]\n",
      " [175.26585 ]\n",
      " [157.49255 ]]\n",
      "=================\n",
      "R2 :  0.3834339238408203\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 28.2993\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.8564\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.6521\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.2346\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.9368\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.2198\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.1374\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1515\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.0903\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1249\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.1237\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.5967\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 27.9129\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.8714\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1115\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.5037\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.9836\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.2472\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.0050\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.2328\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.6085\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.0823\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.3283\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.6063\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 27.6958\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.5033\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1021\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.7527\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.0022\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.9437\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.5410\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.4381\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1974\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 26.8886\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 27.6665\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.3965\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.3741\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 29.3350\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.0081\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.4831\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.7866\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.4430\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.2945\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 24.0487\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 30.8087\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.6787\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.5080\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.4174\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.4469\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.7085\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.4651\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.9174\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.3577\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.9899\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 26.6151\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.1730\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.7908\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.4477\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.9556\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0463\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.7477\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.1352\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.7268\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.5951\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.1480\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.9345\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.0668\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.5872\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.4531\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8550\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.2019\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.8307\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.4832\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 28.5144\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.6824\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 25.3260\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.5083\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.5382\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 25.5382\n",
      "5/5 [==============================] - 0s 997us/step - loss: 48.5511\n",
      "loss :  48.551124572753906\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[189.99763 ]\n",
      " [ 86.913475]\n",
      " [185.61057 ]\n",
      " [150.45377 ]\n",
      " [166.83308 ]\n",
      " [242.36792 ]\n",
      " [264.32074 ]\n",
      " [ 73.39861 ]\n",
      " [171.63538 ]\n",
      " [187.29233 ]\n",
      " [175.86272 ]\n",
      " [181.38882 ]\n",
      " [150.08595 ]\n",
      " [262.67438 ]\n",
      " [249.77011 ]\n",
      " [169.74188 ]\n",
      " [ 73.39861 ]\n",
      " [152.72247 ]\n",
      " [187.0199  ]\n",
      " [221.65381 ]\n",
      " [198.21548 ]\n",
      " [276.6327  ]\n",
      " [128.79378 ]\n",
      " [ 73.39861 ]\n",
      " [ 73.39861 ]\n",
      " [168.12048 ]\n",
      " [ 95.07866 ]\n",
      " [120.00707 ]\n",
      " [156.22469 ]\n",
      " [168.93239 ]\n",
      " [106.83334 ]\n",
      " [256.29315 ]\n",
      " [190.96306 ]\n",
      " [244.35654 ]\n",
      " [192.19556 ]\n",
      " [ 97.585014]\n",
      " [ 61.18443 ]\n",
      " [131.579   ]\n",
      " [277.05807 ]\n",
      " [114.873375]\n",
      " [252.02458 ]\n",
      " [ 73.39861 ]\n",
      " [118.361786]\n",
      " [ 87.48556 ]\n",
      " [108.73624 ]\n",
      " [262.29846 ]\n",
      " [ 74.70776 ]\n",
      " [ 97.86019 ]\n",
      " [ 51.53106 ]\n",
      " [263.90073 ]\n",
      " [151.58896 ]\n",
      " [101.6722  ]\n",
      " [200.89214 ]\n",
      " [201.35057 ]\n",
      " [216.008   ]\n",
      " [234.3545  ]\n",
      " [222.48795 ]\n",
      " [177.02733 ]\n",
      " [222.19743 ]\n",
      " [171.92368 ]\n",
      " [110.3121  ]\n",
      " [108.123245]\n",
      " [230.08911 ]\n",
      " [217.93866 ]\n",
      " [207.97513 ]\n",
      " [ 51.665466]\n",
      " [ 51.53106 ]\n",
      " [112.005875]\n",
      " [101.75618 ]\n",
      " [ 73.39861 ]\n",
      " [ 81.75338 ]\n",
      " [ 60.347015]\n",
      " [115.12827 ]\n",
      " [226.56079 ]\n",
      " [110.64686 ]\n",
      " [113.306046]\n",
      " [121.87649 ]\n",
      " [131.9758  ]\n",
      " [ 75.620636]\n",
      " [ 73.39861 ]\n",
      " [ 73.39861 ]\n",
      " [124.78137 ]\n",
      " [170.91534 ]\n",
      " [263.82986 ]\n",
      " [231.80371 ]\n",
      " [196.61357 ]\n",
      " [259.3705  ]\n",
      " [ 70.436005]\n",
      " [112.01626 ]\n",
      " [149.03638 ]\n",
      " [159.38719 ]\n",
      " [ 69.73079 ]\n",
      " [200.39883 ]\n",
      " [204.16054 ]\n",
      " [129.90356 ]\n",
      " [272.74014 ]\n",
      " [167.19516 ]\n",
      " [225.99698 ]\n",
      " [119.01794 ]\n",
      " [146.82178 ]\n",
      " [ 75.10559 ]\n",
      " [ 84.570015]\n",
      " [139.07858 ]\n",
      " [257.8464  ]\n",
      " [164.35356 ]\n",
      " [ 73.39861 ]\n",
      " [121.54751 ]\n",
      " [140.20912 ]\n",
      " [197.91779 ]\n",
      " [252.72357 ]\n",
      " [ 89.08596 ]\n",
      " [145.14032 ]\n",
      " [ 56.47108 ]\n",
      " [ 73.39861 ]\n",
      " [ 65.81081 ]\n",
      " [ 99.59524 ]\n",
      " [229.59317 ]\n",
      " [ 73.39861 ]\n",
      " [252.01343 ]\n",
      " [221.27817 ]\n",
      " [138.76884 ]\n",
      " [145.71434 ]\n",
      " [100.885544]\n",
      " [188.26988 ]\n",
      " [194.55177 ]\n",
      " [179.43112 ]\n",
      " [290.85724 ]\n",
      " [235.45013 ]\n",
      " [ 59.601738]\n",
      " [144.92793 ]\n",
      " [ 80.99706 ]\n",
      " [175.16022 ]\n",
      " [182.89893 ]]\n",
      "=================\n",
      "R2 :  0.3423403404122084\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.5893\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.0968\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3176\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.7767\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.6023\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 25.3745\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 27.1662\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.6176\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8405\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.2492\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.1863\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.6833\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.1402\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.6216\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.1328\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.2879\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.8725\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.2101\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.7902\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.3598\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.6442\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.8248\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.5965\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.2708\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.5940\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.0728\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3804\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.3814\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.9810\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.8047\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.7490\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.2955\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.9263\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8708\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3222\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.4818\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.6429\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.5555\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.9336\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.0190\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9557\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4179\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8162\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 26.3638\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 25.7191\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.4814\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.0104\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.4784\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.5129\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.7066\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.7795\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.8794\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3113\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 23.9447\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.9175\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1919\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8987\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.6168\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3849\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.7403\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.9050\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.6314\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3783\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3706\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.2515\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.2223\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.9811\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.9409\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 24.1480\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.9277\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3692\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.9927\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 25.2047\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.0259\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 23.6863\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 24.0121\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.0920\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.2613\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 25.2613\n",
      "5/5 [==============================] - 0s 997us/step - loss: 49.9891\n",
      "loss :  49.98905563354492\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[166.70757 ]\n",
      " [ 85.705475]\n",
      " [201.05219 ]\n",
      " [134.22708 ]\n",
      " [149.99025 ]\n",
      " [217.07123 ]\n",
      " [278.64944 ]\n",
      " [ 74.76534 ]\n",
      " [171.64801 ]\n",
      " [196.78964 ]\n",
      " [151.86078 ]\n",
      " [148.3873  ]\n",
      " [ 84.75054 ]\n",
      " [268.64404 ]\n",
      " [235.47885 ]\n",
      " [116.07423 ]\n",
      " [ 63.967297]\n",
      " [153.64476 ]\n",
      " [181.56299 ]\n",
      " [188.92308 ]\n",
      " [174.74303 ]\n",
      " [249.46504 ]\n",
      " [113.63164 ]\n",
      " [ 50.60554 ]\n",
      " [ 66.00973 ]\n",
      " [163.80577 ]\n",
      " [101.959984]\n",
      " [112.69602 ]\n",
      " [158.77417 ]\n",
      " [164.96317 ]\n",
      " [ 88.25753 ]\n",
      " [258.7659  ]\n",
      " [196.13269 ]\n",
      " [231.1744  ]\n",
      " [187.71431 ]\n",
      " [ 94.9126  ]\n",
      " [ 56.81461 ]\n",
      " [128.747   ]\n",
      " [256.34192 ]\n",
      " [102.73904 ]\n",
      " [244.08109 ]\n",
      " [ 74.76534 ]\n",
      " [113.87396 ]\n",
      " [ 85.682625]\n",
      " [112.82172 ]\n",
      " [259.3052  ]\n",
      " [ 76.154945]\n",
      " [ 81.61245 ]\n",
      " [ 50.60554 ]\n",
      " [232.05437 ]\n",
      " [128.33347 ]\n",
      " [109.41229 ]\n",
      " [199.40587 ]\n",
      " [183.90195 ]\n",
      " [192.83995 ]\n",
      " [216.82077 ]\n",
      " [207.67378 ]\n",
      " [183.53577 ]\n",
      " [223.85107 ]\n",
      " [154.5204  ]\n",
      " [115.371216]\n",
      " [101.48031 ]\n",
      " [213.44719 ]\n",
      " [225.51486 ]\n",
      " [156.4365  ]\n",
      " [ 50.60554 ]\n",
      " [ 54.01161 ]\n",
      " [107.41293 ]\n",
      " [106.68543 ]\n",
      " [ 74.76534 ]\n",
      " [ 71.34544 ]\n",
      " [ 50.60554 ]\n",
      " [144.48505 ]\n",
      " [219.35072 ]\n",
      " [ 98.40096 ]\n",
      " [ 69.310036]\n",
      " [116.39873 ]\n",
      " [ 98.42003 ]\n",
      " [ 74.12374 ]\n",
      " [ 74.76534 ]\n",
      " [ 74.76534 ]\n",
      " [115.97021 ]\n",
      " [150.75502 ]\n",
      " [270.92557 ]\n",
      " [249.1463  ]\n",
      " [173.8965  ]\n",
      " [265.888   ]\n",
      " [ 74.76534 ]\n",
      " [106.66094 ]\n",
      " [125.792496]\n",
      " [151.72139 ]\n",
      " [ 71.64616 ]\n",
      " [199.90216 ]\n",
      " [195.40492 ]\n",
      " [116.4758  ]\n",
      " [299.57935 ]\n",
      " [161.6407  ]\n",
      " [251.93599 ]\n",
      " [123.20926 ]\n",
      " [121.04618 ]\n",
      " [ 78.96697 ]\n",
      " [ 79.26081 ]\n",
      " [144.0236  ]\n",
      " [241.56235 ]\n",
      " [142.27086 ]\n",
      " [ 72.28544 ]\n",
      " [114.84549 ]\n",
      " [128.84254 ]\n",
      " [166.37503 ]\n",
      " [229.72745 ]\n",
      " [ 92.52612 ]\n",
      " [124.70197 ]\n",
      " [ 65.79968 ]\n",
      " [ 63.27403 ]\n",
      " [ 62.615574]\n",
      " [100.866196]\n",
      " [220.50438 ]\n",
      " [ 74.76534 ]\n",
      " [235.03705 ]\n",
      " [168.20695 ]\n",
      " [127.83855 ]\n",
      " [140.91522 ]\n",
      " [ 93.17286 ]\n",
      " [178.60106 ]\n",
      " [189.60933 ]\n",
      " [ 98.92448 ]\n",
      " [270.21204 ]\n",
      " [239.81723 ]\n",
      " [ 58.429756]\n",
      " [144.7887  ]\n",
      " [ 77.72962 ]\n",
      " [170.9181  ]\n",
      " [182.39368 ]]\n",
      "=================\n",
      "R2 :  0.3009544918059167\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.3148\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.5217\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.8597\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.9993\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 24.2239\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2182\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0815\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0713\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3228\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2744\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7687\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 25.1054\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.5101\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2110\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.7791\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0905\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0269\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9362\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.5143\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.1390\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.2325\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7521\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6766\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2632\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.7322\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1880\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7041\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.8524\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.1673\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.0530\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4848\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4344\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2730\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.7950\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4554\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.1100\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.4646\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0847\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3175\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3486\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 24.3184\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4124\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.6617\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.8157\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7506\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9435\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.9329\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.8086\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0351\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0305\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1643\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6801\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3701\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0868\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2269\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0966\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.5293\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4639\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.6212\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4792\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9235\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9166\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0394\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4067\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.2916\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2915\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.6361\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.3062\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.3534\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0824\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.5033\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.1176\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.1301\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7730\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8564\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1794\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6722\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1716\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 100us/step - loss: 23.1716\n",
      "5/5 [==============================] - 0s 748us/step - loss: 51.7792\n",
      "loss :  51.77920913696289\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[170.76707 ]\n",
      " [ 88.21745 ]\n",
      " [200.13672 ]\n",
      " [ 92.40216 ]\n",
      " [147.63524 ]\n",
      " [213.25494 ]\n",
      " [286.6389  ]\n",
      " [ 71.81994 ]\n",
      " [166.91602 ]\n",
      " [179.23868 ]\n",
      " [148.50143 ]\n",
      " [161.7161  ]\n",
      " [ 77.33386 ]\n",
      " [285.80786 ]\n",
      " [214.2262  ]\n",
      " [117.38547 ]\n",
      " [ 71.81994 ]\n",
      " [151.59077 ]\n",
      " [186.56639 ]\n",
      " [195.4973  ]\n",
      " [170.32385 ]\n",
      " [265.92935 ]\n",
      " [109.65993 ]\n",
      " [ 49.112083]\n",
      " [ 71.81994 ]\n",
      " [160.75836 ]\n",
      " [104.068985]\n",
      " [118.23828 ]\n",
      " [156.5041  ]\n",
      " [159.62585 ]\n",
      " [ 82.23474 ]\n",
      " [273.4849  ]\n",
      " [187.70778 ]\n",
      " [247.53435 ]\n",
      " [185.33438 ]\n",
      " [ 80.92574 ]\n",
      " [ 67.24892 ]\n",
      " [132.86664 ]\n",
      " [263.15552 ]\n",
      " [101.38215 ]\n",
      " [250.22278 ]\n",
      " [ 71.81994 ]\n",
      " [121.49791 ]\n",
      " [ 88.132576]\n",
      " [113.58527 ]\n",
      " [258.85117 ]\n",
      " [ 75.40426 ]\n",
      " [ 93.487465]\n",
      " [ 49.112083]\n",
      " [236.13264 ]\n",
      " [126.6293  ]\n",
      " [116.483215]\n",
      " [192.57408 ]\n",
      " [186.9037  ]\n",
      " [191.77364 ]\n",
      " [215.1401  ]\n",
      " [227.53928 ]\n",
      " [187.78842 ]\n",
      " [216.42993 ]\n",
      " [151.082   ]\n",
      " [118.00658 ]\n",
      " [ 98.16828 ]\n",
      " [228.36403 ]\n",
      " [252.34421 ]\n",
      " [169.29791 ]\n",
      " [ 49.112083]\n",
      " [ 49.112083]\n",
      " [ 87.91124 ]\n",
      " [119.51194 ]\n",
      " [ 71.81994 ]\n",
      " [ 71.81994 ]\n",
      " [ 49.112083]\n",
      " [142.3527  ]\n",
      " [204.4213  ]\n",
      " [101.617744]\n",
      " [ 49.696423]\n",
      " [136.59727 ]\n",
      " [ 85.95074 ]\n",
      " [ 69.24305 ]\n",
      " [ 71.81994 ]\n",
      " [ 71.81994 ]\n",
      " [115.73151 ]\n",
      " [140.28761 ]\n",
      " [280.4065  ]\n",
      " [256.66238 ]\n",
      " [168.2359  ]\n",
      " [285.07086 ]\n",
      " [ 71.59477 ]\n",
      " [ 97.54054 ]\n",
      " [105.723434]\n",
      " [132.65938 ]\n",
      " [ 59.485558]\n",
      " [195.21664 ]\n",
      " [192.09337 ]\n",
      " [112.67442 ]\n",
      " [288.744   ]\n",
      " [162.10219 ]\n",
      " [279.89478 ]\n",
      " [123.38051 ]\n",
      " [110.2618  ]\n",
      " [ 75.01321 ]\n",
      " [ 75.90971 ]\n",
      " [143.02245 ]\n",
      " [246.6127  ]\n",
      " [136.19064 ]\n",
      " [ 71.81994 ]\n",
      " [120.104904]\n",
      " [ 89.91465 ]\n",
      " [171.16049 ]\n",
      " [235.81369 ]\n",
      " [ 88.07677 ]\n",
      " [118.75755 ]\n",
      " [ 49.112083]\n",
      " [ 71.81994 ]\n",
      " [ 71.16347 ]\n",
      " [112.34124 ]\n",
      " [228.88658 ]\n",
      " [ 71.81994 ]\n",
      " [243.02492 ]\n",
      " [177.2701  ]\n",
      " [128.96613 ]\n",
      " [123.86587 ]\n",
      " [101.529526]\n",
      " [172.73428 ]\n",
      " [184.89378 ]\n",
      " [101.78401 ]\n",
      " [281.82718 ]\n",
      " [261.02847 ]\n",
      " [ 49.724407]\n",
      " [138.42166 ]\n",
      " [ 89.41197 ]\n",
      " [161.93105 ]\n",
      " [178.78242 ]]\n",
      "=================\n",
      "R2 :  0.2362615626815664\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2781\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.9595\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.5407\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.1250\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.2931\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.4023\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.1886\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.4863\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.4017\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4388\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0823\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6430\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8371\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.8311\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.5533\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.8757\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8874\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.1889\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0541\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0602\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.9046\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.1595\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.1190\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0465\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.8208\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0742\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1773\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 23.8431\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.8275\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4098\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8155\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.7231\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.2494\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 25.0629\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7565\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3939\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.3505\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4279\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0710\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4694\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.9774\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.0689\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 24.8711\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4206\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5625\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.9652\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.4974\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.3868\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2974\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4689\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.6301\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2488\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3985\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.9653\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0733\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7215\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6139\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5009\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.5582\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7160\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7598\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2089\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0393\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.9347\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.7625\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0670\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5594\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.5949\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5687\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7990\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5733\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.3127\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.5757\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3563\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7268\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 23.0207\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.4234\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.4250\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 20.4250\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 54.1656\n",
      "loss :  54.165584564208984\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[169.67506 ]\n",
      " [ 93.95065 ]\n",
      " [186.08977 ]\n",
      " [ 83.882095]\n",
      " [148.53513 ]\n",
      " [221.29398 ]\n",
      " [272.70825 ]\n",
      " [ 77.88156 ]\n",
      " [179.47862 ]\n",
      " [222.93665 ]\n",
      " [138.33821 ]\n",
      " [156.41136 ]\n",
      " [ 55.508957]\n",
      " [291.9349  ]\n",
      " [210.22728 ]\n",
      " [146.92294 ]\n",
      " [ 70.20865 ]\n",
      " [153.42014 ]\n",
      " [181.4753  ]\n",
      " [197.5949  ]\n",
      " [188.05972 ]\n",
      " [261.6912  ]\n",
      " [123.26064 ]\n",
      " [ 50.24606 ]\n",
      " [ 77.88156 ]\n",
      " [142.14479 ]\n",
      " [116.65112 ]\n",
      " [101.11429 ]\n",
      " [154.21516 ]\n",
      " [167.97435 ]\n",
      " [ 95.635315]\n",
      " [280.68085 ]\n",
      " [193.20787 ]\n",
      " [238.63011 ]\n",
      " [186.52756 ]\n",
      " [ 80.92152 ]\n",
      " [ 77.88156 ]\n",
      " [138.06903 ]\n",
      " [254.12741 ]\n",
      " [101.045135]\n",
      " [245.0869  ]\n",
      " [ 77.88156 ]\n",
      " [135.34483 ]\n",
      " [103.3453  ]\n",
      " [123.963486]\n",
      " [240.79807 ]\n",
      " [ 84.0072  ]\n",
      " [106.23958 ]\n",
      " [ 50.24606 ]\n",
      " [221.62755 ]\n",
      " [131.61243 ]\n",
      " [117.882706]\n",
      " [195.13518 ]\n",
      " [180.19614 ]\n",
      " [208.0915  ]\n",
      " [212.60184 ]\n",
      " [227.34044 ]\n",
      " [202.06598 ]\n",
      " [225.82246 ]\n",
      " [144.09561 ]\n",
      " [123.5828  ]\n",
      " [116.13712 ]\n",
      " [209.06291 ]\n",
      " [257.14517 ]\n",
      " [149.81871 ]\n",
      " [ 50.24606 ]\n",
      " [ 52.525585]\n",
      " [104.5797  ]\n",
      " [119.65642 ]\n",
      " [ 77.88156 ]\n",
      " [ 80.87055 ]\n",
      " [ 50.24606 ]\n",
      " [119.00331 ]\n",
      " [208.20236 ]\n",
      " [107.721375]\n",
      " [ 55.964893]\n",
      " [128.75429 ]\n",
      " [ 82.03233 ]\n",
      " [ 72.35154 ]\n",
      " [ 77.88156 ]\n",
      " [ 77.88156 ]\n",
      " [123.25813 ]\n",
      " [159.05795 ]\n",
      " [280.96774 ]\n",
      " [252.58289 ]\n",
      " [152.90617 ]\n",
      " [288.19638 ]\n",
      " [ 77.88156 ]\n",
      " [110.41652 ]\n",
      " [ 91.59445 ]\n",
      " [135.78535 ]\n",
      " [ 69.13568 ]\n",
      " [200.0529  ]\n",
      " [204.94656 ]\n",
      " [115.20061 ]\n",
      " [296.5335  ]\n",
      " [182.4823  ]\n",
      " [309.38568 ]\n",
      " [126.431145]\n",
      " [116.8883  ]\n",
      " [ 84.15571 ]\n",
      " [ 85.127655]\n",
      " [145.50536 ]\n",
      " [249.26512 ]\n",
      " [141.55371 ]\n",
      " [ 82.66132 ]\n",
      " [116.35608 ]\n",
      " [ 73.58771 ]\n",
      " [161.54137 ]\n",
      " [210.72237 ]\n",
      " [ 73.03476 ]\n",
      " [129.6563  ]\n",
      " [ 51.14888 ]\n",
      " [ 77.88156 ]\n",
      " [ 88.34204 ]\n",
      " [ 61.48359 ]\n",
      " [215.48383 ]\n",
      " [ 77.88156 ]\n",
      " [224.75357 ]\n",
      " [143.07776 ]\n",
      " [135.13211 ]\n",
      " [112.53621 ]\n",
      " [106.14997 ]\n",
      " [170.25148 ]\n",
      " [184.36981 ]\n",
      " [103.1317  ]\n",
      " [287.8239  ]\n",
      " [257.91174 ]\n",
      " [ 77.88156 ]\n",
      " [130.36415 ]\n",
      " [ 84.432976]\n",
      " [145.83344 ]\n",
      " [191.18727 ]]\n",
      "=================\n",
      "R2 :  0.1706586861181233\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.0635\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6893\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.5543\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1812\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.0826\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.0559\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.0725\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5191\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5429\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7250\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3089\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0581\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1096\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.7299\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5431\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3598\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1027\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5826\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6633\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.3776\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6527\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1338\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.8208\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5151\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.5095\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4713\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2892\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1275\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.0826\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 18.3685\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.4313\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.9822\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5539\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4095\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.3410\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.5211\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3273\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4214\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6424\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8212\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7028\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 19.6426\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.0206\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.1690\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4053\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 23.6450\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.2794\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5673\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7371\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.1103\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3615\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1653\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2478\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.4098\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2800\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8668\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.1800\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.3608\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.6831\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.5133\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5674\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.7790\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.6978\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.3241\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2228\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 18.9676\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.0933\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.5746\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4482\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.5951\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8085\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.8912\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.3848\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.9009\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.2709\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2923\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.6059\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.5004\n",
      "Epoch 79/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 130us/step - loss: 19.5004\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 52.7599\n",
      "loss :  52.75987243652344\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[185. 150. 246. 184. 110. 202. 336.  69.  69.  87.  66. 164. 265. 198.\n",
      " 248. 171. 102. 182. 262. 233. 151. 275. 230.  39.  42. 122.  81.  31.\n",
      " 156. 161.  80. 215. 310. 163. 265.  84.  54. 137. 248.  72. 248.  59.\n",
      "  97.  61.  72. 296.  55.  75.  53. 173. 158.  96. 140. 186.  93. 123.\n",
      " 237. 138. 292.  78. 182. 127. 321. 109. 212.  45.  96. 109.  89.  96.\n",
      "  60.  51. 200. 265.  68.  91. 172. 115. 125.  53.  63. 259. 214. 195.\n",
      " 258. 235. 173.  59. 219. 170.  77.  49.  66. 144. 113. 233. 162. 121.\n",
      "  88. 302. 128.  52. 178.  90. 164. 135. 103. 200. 178. 261.  64.  59.\n",
      "  79.  47. 107.  39. 151. 104. 217. 232.  55. 245. 131.  64. 222. 249.\n",
      " 128. 293. 138. 273. 158. 168. 103.]\n",
      "[[169.01178 ]\n",
      " [ 88.53383 ]\n",
      " [180.8958  ]\n",
      " [ 85.250565]\n",
      " [146.78513 ]\n",
      " [224.0789  ]\n",
      " [264.06686 ]\n",
      " [ 71.589745]\n",
      " [178.9899  ]\n",
      " [199.95319 ]\n",
      " [150.82367 ]\n",
      " [154.26378 ]\n",
      " [ 89.16513 ]\n",
      " [283.106   ]\n",
      " [194.55276 ]\n",
      " [148.69992 ]\n",
      " [ 48.50066 ]\n",
      " [142.77632 ]\n",
      " [167.05145 ]\n",
      " [201.29878 ]\n",
      " [184.08815 ]\n",
      " [253.98622 ]\n",
      " [113.480446]\n",
      " [ 48.50066 ]\n",
      " [ 71.589745]\n",
      " [174.0952  ]\n",
      " [109.8975  ]\n",
      " [106.90508 ]\n",
      " [145.11134 ]\n",
      " [157.32224 ]\n",
      " [ 80.267   ]\n",
      " [270.37698 ]\n",
      " [178.37257 ]\n",
      " [249.70181 ]\n",
      " [172.33427 ]\n",
      " [ 95.84383 ]\n",
      " [ 67.07199 ]\n",
      " [127.15715 ]\n",
      " [249.74876 ]\n",
      " [ 97.87742 ]\n",
      " [227.10663 ]\n",
      " [ 65.25105 ]\n",
      " [132.15016 ]\n",
      " [ 96.24369 ]\n",
      " [117.32777 ]\n",
      " [242.52126 ]\n",
      " [ 77.36769 ]\n",
      " [106.83515 ]\n",
      " [ 48.50066 ]\n",
      " [209.73354 ]\n",
      " [115.75539 ]\n",
      " [107.15925 ]\n",
      " [181.50566 ]\n",
      " [173.72185 ]\n",
      " [198.6232  ]\n",
      " [200.47154 ]\n",
      " [239.19989 ]\n",
      " [202.08311 ]\n",
      " [208.48721 ]\n",
      " [135.16743 ]\n",
      " [111.23559 ]\n",
      " [126.355606]\n",
      " [233.10101 ]\n",
      " [256.23337 ]\n",
      " [159.64145 ]\n",
      " [ 48.50066 ]\n",
      " [ 52.65691 ]\n",
      " [115.48192 ]\n",
      " [118.5216  ]\n",
      " [ 71.589745]\n",
      " [ 71.589745]\n",
      " [ 48.50066 ]\n",
      " [175.819   ]\n",
      " [189.1633  ]\n",
      " [103.028114]\n",
      " [ 65.90129 ]\n",
      " [133.14502 ]\n",
      " [ 73.83172 ]\n",
      " [ 66.60374 ]\n",
      " [ 71.589745]\n",
      " [ 71.589745]\n",
      " [126.84501 ]\n",
      " [163.95293 ]\n",
      " [270.88266 ]\n",
      " [249.77383 ]\n",
      " [172.12672 ]\n",
      " [283.80066 ]\n",
      " [ 66.4298  ]\n",
      " [114.76157 ]\n",
      " [ 99.66916 ]\n",
      " [142.82687 ]\n",
      " [ 48.50066 ]\n",
      " [184.89389 ]\n",
      " [202.65929 ]\n",
      " [102.54715 ]\n",
      " [299.76575 ]\n",
      " [157.4372  ]\n",
      " [311.02716 ]\n",
      " [119.22984 ]\n",
      " [102.75589 ]\n",
      " [ 71.585396]\n",
      " [ 71.99616 ]\n",
      " [139.11542 ]\n",
      " [223.69936 ]\n",
      " [133.11116 ]\n",
      " [ 73.74701 ]\n",
      " [107.50509 ]\n",
      " [ 77.02226 ]\n",
      " [178.50945 ]\n",
      " [224.69737 ]\n",
      " [ 63.999393]\n",
      " [118.97852 ]\n",
      " [ 48.50066 ]\n",
      " [ 71.589745]\n",
      " [ 92.05289 ]\n",
      " [ 68.11193 ]\n",
      " [225.18947 ]\n",
      " [ 71.589745]\n",
      " [261.9744  ]\n",
      " [164.13103 ]\n",
      " [113.89845 ]\n",
      " [119.063896]\n",
      " [ 87.466965]\n",
      " [153.01503 ]\n",
      " [176.46626 ]\n",
      " [ 83.36206 ]\n",
      " [279.73062 ]\n",
      " [259.29565 ]\n",
      " [ 60.47837 ]\n",
      " [122.56928 ]\n",
      " [101.13108 ]\n",
      " [132.6159  ]\n",
      " [180.24628 ]]\n",
      "=================\n",
      "R2 :  0.21002715560717655\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2462\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.5697\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 18.7416\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.4434\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2655\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 18.3597\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.8897\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 18.8997\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7270\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.6069\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 22.2488\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 20.2579\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.3917\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.8434\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.2101\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6851\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 19.9682\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.0825\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6441\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.2732\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.2780\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.4211\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 22.1481\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.0198\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.6892\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.1817\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.8774\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 21.2203\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.3710\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 19.9864\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 18.9906\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.4031\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1335\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.7493\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.9740\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.3146\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.7443\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 18.8567\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 20.1389\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 21.1339\n",
      "Epoch 41/100\n",
      " 43/100 [===========>..................] - ETA: 0s - loss: 18.5982"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5676\\2344056446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 4. 모델 컴파일\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.compile(loss=\"mae\",optimizer=\"adam\")\n",
    "f = open(\"C:\\study\\keras\\diabets.txt\",'a')\n",
    "\n",
    "# 4. 모델 컴파일\n",
    "while (True):\n",
    "    model.fit(x_train,y_train,epochs=100,batch_size=4,steps_per_epoch=100)\n",
    "    loss = model.evaluate(x_test,y_test)\n",
    "    print(\"loss : \",loss)\n",
    "    y_predict =model.predict(x_test)\n",
    "    print(\"=================\")\n",
    "    print(y_test)\n",
    "    print(y_predict)\n",
    "    print(\"=================\")\n",
    "    r2 = r2_score(y_test,y_predict)\n",
    "    print(\"R2 : \",r2)\n",
    "    \n",
    "    f.write(str(datetime.now())+str(r2)+\"\\n\") \n",
    "    \n",
    "    if r2 >= 0.62 :\n",
    "        f.write(str(datetime.now())+str(r2)+\"\\n\") \n",
    "        model.save(\"diabets.h5\")\n",
    "        f.close()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
