{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n",
      "2.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "print(sk.__version__)\n",
    "print(tf.__version__)\n",
    "dataset = load_boston()\n",
    "x = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# 13개의 칼럼 을 가지고있는 데이터를 조회합니다\n",
    "print(x.shape) #(506,13)\n",
    "# print(x)\n",
    "print(y.shape) #(506,)\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# 사이킥 런의 칼럼 조회기능\n",
    "print(dataset.feature_names)\n",
    "print(len(dataset.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사이킥런의 데이터 요약\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델구성\n",
    "# 실습 train 0.7 이상\n",
    "# 평가지표 R2 : 0.8 이상으로 뽑아내보기 / RMSE 사용\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,\n",
    "    train_size=0.7,\n",
    "    shuffle = True,\n",
    "    random_state= 123\n",
    ")\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_54 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,911\n",
      "Trainable params: 1,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "number1 = randrange(1,10,1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10,input_dim = 13),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 2ms/step - loss: 14.6400\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 6.9422\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.8148\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 6.0078\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.8411\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.3947\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.3466\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.3454\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.4071\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.5712\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.4577\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.0352\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.4245\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8618\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.1254\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.1481\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.1030\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8802\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.9773\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.9815\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8894\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.9295\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.6527\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8040\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.5155\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4762\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4559\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8335\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4283\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.3739\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.6164\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.2988\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.3335\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.5632\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4007\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.1867\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.2754\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.9909\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.1341\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.1735\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.2588\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.0885\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.6919\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7466\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.9184\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.9106\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.0542\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.8081\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.8875\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.9472\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.8242\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.2351\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6122\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.8924\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.5075\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7572\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.8487\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.9100\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4304\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7209\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5432\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5107\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5751\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4199\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7794\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.4135\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5670\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4917\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4407\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2738\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4121\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3868\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3672\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2737\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3469\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4083\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3820\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9558\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3175\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1347\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0635\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1202\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1889\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1372\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0910\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9096\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1257\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8527\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4062\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 3.4062\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.6921\n",
      "loss :  3.692131519317627\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[ 9.662329 ]\n",
      " [26.984997 ]\n",
      " [42.258656 ]\n",
      " [13.642284 ]\n",
      " [29.118773 ]\n",
      " [49.094128 ]\n",
      " [28.96745  ]\n",
      " [12.764276 ]\n",
      " [20.140589 ]\n",
      " [26.381113 ]\n",
      " [26.53761  ]\n",
      " [23.355927 ]\n",
      " [17.941792 ]\n",
      " [36.14031  ]\n",
      " [19.041866 ]\n",
      " [22.238861 ]\n",
      " [22.92902  ]\n",
      " [38.5065   ]\n",
      " [21.408545 ]\n",
      " [13.813165 ]\n",
      " [19.658634 ]\n",
      " [26.591763 ]\n",
      " [35.91175  ]\n",
      " [45.02312  ]\n",
      " [52.27064  ]\n",
      " [24.392742 ]\n",
      " [13.39167  ]\n",
      " [24.235855 ]\n",
      " [23.436249 ]\n",
      " [22.960957 ]\n",
      " [25.99202  ]\n",
      " [37.614094 ]\n",
      " [10.199217 ]\n",
      " [21.924248 ]\n",
      " [24.40324  ]\n",
      " [27.913572 ]\n",
      " [27.359497 ]\n",
      " [17.038324 ]\n",
      " [18.607702 ]\n",
      " [47.923565 ]\n",
      " [28.494503 ]\n",
      " [20.581049 ]\n",
      " [20.136547 ]\n",
      " [57.02282  ]\n",
      " [19.639427 ]\n",
      " [26.02619  ]\n",
      " [24.500229 ]\n",
      " [25.13835  ]\n",
      " [21.144438 ]\n",
      " [20.925732 ]\n",
      " [39.462765 ]\n",
      " [23.101807 ]\n",
      " [24.560362 ]\n",
      " [14.170162 ]\n",
      " [20.466457 ]\n",
      " [16.486221 ]\n",
      " [15.09753  ]\n",
      " [11.552114 ]\n",
      " [29.81144  ]\n",
      " [15.979419 ]\n",
      " [21.958963 ]\n",
      " [23.81255  ]\n",
      " [15.835353 ]\n",
      " [18.067974 ]\n",
      " [23.06426  ]\n",
      " [25.88179  ]\n",
      " [27.203861 ]\n",
      " [14.191637 ]\n",
      " [28.025097 ]\n",
      " [29.67235  ]\n",
      " [21.342827 ]\n",
      " [25.715721 ]\n",
      " [20.495834 ]\n",
      " [27.240246 ]\n",
      " [17.224466 ]\n",
      " [20.047573 ]\n",
      " [10.10617  ]\n",
      " [21.70205  ]\n",
      " [32.84431  ]\n",
      " [ 8.858489 ]\n",
      " [29.637379 ]\n",
      " [11.443675 ]\n",
      " [26.540575 ]\n",
      " [21.739506 ]\n",
      " [19.53119  ]\n",
      " [26.725931 ]\n",
      " [19.379435 ]\n",
      " [24.316746 ]\n",
      " [22.302757 ]\n",
      " [30.350372 ]\n",
      " [13.957248 ]\n",
      " [31.846752 ]\n",
      " [22.638699 ]\n",
      " [27.313522 ]\n",
      " [28.287558 ]\n",
      " [28.544945 ]\n",
      " [16.820013 ]\n",
      " [32.296906 ]\n",
      " [29.530792 ]\n",
      " [36.684174 ]\n",
      " [22.47313  ]\n",
      " [13.924806 ]\n",
      " [41.016712 ]\n",
      " [15.9587145]\n",
      " [22.544323 ]\n",
      " [24.604763 ]\n",
      " [19.681906 ]\n",
      " [15.826435 ]\n",
      " [19.571135 ]\n",
      " [24.540497 ]\n",
      " [23.803638 ]\n",
      " [25.120087 ]\n",
      " [18.979254 ]\n",
      " [21.918169 ]\n",
      " [13.970712 ]\n",
      " [26.534615 ]\n",
      " [34.54573  ]\n",
      " [10.387033 ]\n",
      " [20.783443 ]\n",
      " [21.238607 ]\n",
      " [26.939508 ]\n",
      " [39.709766 ]\n",
      " [17.673016 ]\n",
      " [13.487794 ]\n",
      " [18.847424 ]\n",
      " [22.593384 ]\n",
      " [21.443094 ]\n",
      " [25.743238 ]\n",
      " [18.511684 ]\n",
      " [20.998264 ]\n",
      " [22.108456 ]\n",
      " [12.953614 ]\n",
      " [14.579795 ]\n",
      " [ 9.262334 ]\n",
      " [17.400764 ]\n",
      " [15.3240385]\n",
      " [21.76973  ]\n",
      " [15.075688 ]\n",
      " [21.636677 ]\n",
      " [21.391071 ]\n",
      " [22.583569 ]\n",
      " [23.280771 ]\n",
      " [13.409614 ]\n",
      " [25.43584  ]\n",
      " [56.164936 ]\n",
      " [16.625198 ]\n",
      " [22.105625 ]\n",
      " [19.052189 ]\n",
      " [25.914547 ]\n",
      " [21.028793 ]\n",
      " [12.1318865]\n",
      " [22.44365  ]]\n",
      "=================\n",
      "R2 :  0.6824067637015396\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8539\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2446\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0505\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9312\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0187\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0463\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9075\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2716\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0680\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2973\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8774\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1623\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2910\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0305\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8548\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9992\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2341\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0762\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3215\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7776\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.0688\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9711\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0048\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7718\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.3265\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9517\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8067\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.1436\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9028\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.1817\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8424\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9871\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.0082\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8327\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6279\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9997\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9268\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7185\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9094\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8216\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8226\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7545\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9339\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9747\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7893\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8253\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1272\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9715\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8362\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8038\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0654\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8706\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0141\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7834\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8244\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7132\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6748\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7101\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8518\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0156\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7730\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9350\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0086\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8811\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9080\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6616\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0312\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8431\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9026\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9257\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9830\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9378\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9089\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8783\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8481\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8256\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6920\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8512\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7446\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8716\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7892\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8535\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7225\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9046\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6233\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0388\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7815\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7726\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7706\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 100us/step - loss: 2.7706\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.1079\n",
      "loss :  3.1079235076904297\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[10.838968 ]\n",
      " [25.045416 ]\n",
      " [42.074078 ]\n",
      " [14.870021 ]\n",
      " [27.981943 ]\n",
      " [48.63998  ]\n",
      " [26.771727 ]\n",
      " [12.491343 ]\n",
      " [18.174353 ]\n",
      " [23.639566 ]\n",
      " [23.968714 ]\n",
      " [21.522028 ]\n",
      " [17.38096  ]\n",
      " [33.060566 ]\n",
      " [19.746407 ]\n",
      " [19.853733 ]\n",
      " [20.82289  ]\n",
      " [36.678806 ]\n",
      " [19.645527 ]\n",
      " [11.867523 ]\n",
      " [18.94657  ]\n",
      " [23.794588 ]\n",
      " [34.82352  ]\n",
      " [45.583057 ]\n",
      " [50.040543 ]\n",
      " [21.94736  ]\n",
      " [13.343231 ]\n",
      " [21.864075 ]\n",
      " [21.172907 ]\n",
      " [23.499704 ]\n",
      " [23.068953 ]\n",
      " [36.833508 ]\n",
      " [ 9.747698 ]\n",
      " [20.39544  ]\n",
      " [21.878906 ]\n",
      " [24.744291 ]\n",
      " [24.095167 ]\n",
      " [14.013745 ]\n",
      " [16.567194 ]\n",
      " [45.532955 ]\n",
      " [27.546041 ]\n",
      " [18.457296 ]\n",
      " [20.444267 ]\n",
      " [54.756187 ]\n",
      " [16.626514 ]\n",
      " [23.199389 ]\n",
      " [21.868246 ]\n",
      " [21.995085 ]\n",
      " [18.74072  ]\n",
      " [17.880148 ]\n",
      " [39.492706 ]\n",
      " [21.789885 ]\n",
      " [21.94237  ]\n",
      " [11.710955 ]\n",
      " [17.913128 ]\n",
      " [16.515097 ]\n",
      " [12.885912 ]\n",
      " [12.349721 ]\n",
      " [30.409937 ]\n",
      " [13.010971 ]\n",
      " [18.826283 ]\n",
      " [21.340647 ]\n",
      " [15.75239  ]\n",
      " [17.240072 ]\n",
      " [21.17945  ]\n",
      " [23.223253 ]\n",
      " [23.912308 ]\n",
      " [13.852746 ]\n",
      " [24.373892 ]\n",
      " [29.660872 ]\n",
      " [18.796446 ]\n",
      " [23.672789 ]\n",
      " [18.20189  ]\n",
      " [23.924154 ]\n",
      " [15.507316 ]\n",
      " [18.917099 ]\n",
      " [ 9.277618 ]\n",
      " [20.357248 ]\n",
      " [29.36299  ]\n",
      " [ 7.4868917]\n",
      " [27.211832 ]\n",
      " [ 9.467363 ]\n",
      " [23.449224 ]\n",
      " [20.248648 ]\n",
      " [17.875685 ]\n",
      " [23.502077 ]\n",
      " [15.719153 ]\n",
      " [22.032984 ]\n",
      " [20.545376 ]\n",
      " [30.964788 ]\n",
      " [13.422499 ]\n",
      " [32.21011  ]\n",
      " [20.269695 ]\n",
      " [24.451014 ]\n",
      " [27.10451  ]\n",
      " [27.202429 ]\n",
      " [13.84701  ]\n",
      " [33.547607 ]\n",
      " [25.257936 ]\n",
      " [36.5249   ]\n",
      " [20.410484 ]\n",
      " [11.266815 ]\n",
      " [42.708996 ]\n",
      " [13.483695 ]\n",
      " [20.111004 ]\n",
      " [22.424406 ]\n",
      " [17.000113 ]\n",
      " [14.113565 ]\n",
      " [18.238977 ]\n",
      " [22.057322 ]\n",
      " [20.885235 ]\n",
      " [22.385096 ]\n",
      " [16.328093 ]\n",
      " [18.653568 ]\n",
      " [13.345417 ]\n",
      " [24.42391  ]\n",
      " [32.17143  ]\n",
      " [10.067856 ]\n",
      " [18.157743 ]\n",
      " [18.509295 ]\n",
      " [23.631914 ]\n",
      " [36.267193 ]\n",
      " [16.881353 ]\n",
      " [12.933776 ]\n",
      " [16.7545   ]\n",
      " [20.919563 ]\n",
      " [19.72877  ]\n",
      " [22.783514 ]\n",
      " [16.188334 ]\n",
      " [20.063808 ]\n",
      " [20.092213 ]\n",
      " [10.614277 ]\n",
      " [13.549742 ]\n",
      " [ 8.759798 ]\n",
      " [18.293728 ]\n",
      " [13.754444 ]\n",
      " [18.679665 ]\n",
      " [12.411976 ]\n",
      " [19.242373 ]\n",
      " [19.760195 ]\n",
      " [19.607376 ]\n",
      " [21.563026 ]\n",
      " [12.631634 ]\n",
      " [22.533098 ]\n",
      " [54.27817  ]\n",
      " [13.82445  ]\n",
      " [19.968725 ]\n",
      " [17.914736 ]\n",
      " [23.670855 ]\n",
      " [18.927488 ]\n",
      " [11.537544 ]\n",
      " [19.883814 ]]\n",
      "=================\n",
      "R2 :  0.7305709543625041\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8313\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8812\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1482\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7947\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6400\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6361\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9958\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9211\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1284\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9558\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6216\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7746\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6944\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8249\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5926\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9087\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6999\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9381\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6602\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7755\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7260\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8317\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7795\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7529\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8476\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9571\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7488\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7222\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8061\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6639\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8832\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6710\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6650\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7213\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0048\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6174\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5914\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7937\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5909\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1027\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6993\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9682\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5681\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8506\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5441\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5021\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6518\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7523\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6109\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6293\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6152\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6084\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3977\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7005\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6885\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7979\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8018\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5633\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4611\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7543\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5066\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7060\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5151\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4527\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9168\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4921\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6823\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6662\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5494\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6240\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6069\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5453\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7168\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.6307\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0445\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4600\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8898\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3035\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4988\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6588\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6729\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4015\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7162\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8344\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6205\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4019\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5861\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8713\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7986\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 2.7986\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.0486\n",
      "loss :  3.0485646724700928\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[10.874943 ]\n",
      " [24.182003 ]\n",
      " [43.79323  ]\n",
      " [15.637259 ]\n",
      " [25.349213 ]\n",
      " [47.24446  ]\n",
      " [27.467684 ]\n",
      " [12.61547  ]\n",
      " [17.512962 ]\n",
      " [22.652761 ]\n",
      " [22.512177 ]\n",
      " [21.039865 ]\n",
      " [17.773645 ]\n",
      " [34.149788 ]\n",
      " [20.823288 ]\n",
      " [19.257725 ]\n",
      " [20.281107 ]\n",
      " [33.499924 ]\n",
      " [19.138979 ]\n",
      " [14.024393 ]\n",
      " [16.196457 ]\n",
      " [26.202366 ]\n",
      " [36.286354 ]\n",
      " [44.28374  ]\n",
      " [47.93109  ]\n",
      " [20.951593 ]\n",
      " [14.227735 ]\n",
      " [20.960522 ]\n",
      " [20.516258 ]\n",
      " [15.718256 ]\n",
      " [22.127571 ]\n",
      " [34.70569  ]\n",
      " [ 8.568783 ]\n",
      " [20.071665 ]\n",
      " [21.05947  ]\n",
      " [24.32292  ]\n",
      " [23.133785 ]\n",
      " [13.377241 ]\n",
      " [16.02396  ]\n",
      " [46.503784 ]\n",
      " [25.550701 ]\n",
      " [17.936726 ]\n",
      " [17.807133 ]\n",
      " [55.11248  ]\n",
      " [15.923315 ]\n",
      " [22.264267 ]\n",
      " [20.96535  ]\n",
      " [20.686474 ]\n",
      " [18.010687 ]\n",
      " [18.89488  ]\n",
      " [35.79921  ]\n",
      " [21.850048 ]\n",
      " [21.080748 ]\n",
      " [10.592123 ]\n",
      " [17.16041  ]\n",
      " [14.922232 ]\n",
      " [13.078445 ]\n",
      " [10.333431 ]\n",
      " [29.818958 ]\n",
      " [12.238438 ]\n",
      " [17.793793 ]\n",
      " [20.304802 ]\n",
      " [12.086663 ]\n",
      " [19.14002  ]\n",
      " [20.829147 ]\n",
      " [22.198565 ]\n",
      " [22.910439 ]\n",
      " [15.594173 ]\n",
      " [23.445333 ]\n",
      " [27.482231 ]\n",
      " [18.096556 ]\n",
      " [23.921043 ]\n",
      " [17.551596 ]\n",
      " [22.922989 ]\n",
      " [14.778147 ]\n",
      " [20.069294 ]\n",
      " [11.715307 ]\n",
      " [19.806553 ]\n",
      " [27.522865 ]\n",
      " [ 7.828395 ]\n",
      " [26.971416 ]\n",
      " [10.232317 ]\n",
      " [22.524208 ]\n",
      " [19.870934 ]\n",
      " [17.443226 ]\n",
      " [22.631834 ]\n",
      " [14.872667 ]\n",
      " [20.981945 ]\n",
      " [20.055122 ]\n",
      " [28.74348  ]\n",
      " [14.01278  ]\n",
      " [30.391003 ]\n",
      " [21.02768  ]\n",
      " [23.040058 ]\n",
      " [24.719097 ]\n",
      " [23.837696 ]\n",
      " [13.413851 ]\n",
      " [30.68453  ]\n",
      " [24.130568 ]\n",
      " [34.570553 ]\n",
      " [19.655876 ]\n",
      " [13.102714 ]\n",
      " [42.83905  ]\n",
      " [13.405281 ]\n",
      " [19.475937 ]\n",
      " [21.72385  ]\n",
      " [16.026133 ]\n",
      " [13.7708435]\n",
      " [19.816767 ]\n",
      " [21.0374   ]\n",
      " [19.78464  ]\n",
      " [21.422535 ]\n",
      " [15.277765 ]\n",
      " [17.620323 ]\n",
      " [14.236671 ]\n",
      " [22.951431 ]\n",
      " [30.976692 ]\n",
      " [12.311914 ]\n",
      " [17.454962 ]\n",
      " [18.995188 ]\n",
      " [24.99163  ]\n",
      " [29.8618   ]\n",
      " [17.433352 ]\n",
      " [13.563219 ]\n",
      " [16.110075 ]\n",
      " [19.570843 ]\n",
      " [19.16269  ]\n",
      " [21.992186 ]\n",
      " [15.055425 ]\n",
      " [21.286243 ]\n",
      " [19.342226 ]\n",
      " [11.758941 ]\n",
      " [14.12883  ]\n",
      " [ 9.74436  ]\n",
      " [12.487688 ]\n",
      " [14.160339 ]\n",
      " [17.789547 ]\n",
      " [11.919021 ]\n",
      " [18.537735 ]\n",
      " [20.621754 ]\n",
      " [18.726612 ]\n",
      " [23.33004  ]\n",
      " [12.323796 ]\n",
      " [21.577602 ]\n",
      " [52.52891  ]\n",
      " [13.516771 ]\n",
      " [20.375454 ]\n",
      " [18.888996 ]\n",
      " [22.392803 ]\n",
      " [17.507582 ]\n",
      " [12.034384 ]\n",
      " [19.53515  ]]\n",
      "=================\n",
      "R2 :  0.7375201059186528\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6345\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8159\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5737\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4586\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4928\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5053\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6067\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5647\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8407\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3859\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5394\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7159\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6628\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4422\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4887\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4153\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5170\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8254\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6504\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8995\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5621\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3206\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5933\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3917\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7434\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2704\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4441\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4900\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4867\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4469\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4721\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4788\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3880\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6767\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2879\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5929\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6384\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4961\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5043\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4633\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4933\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3749\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4430\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2474\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4290\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3401\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3148\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4288\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6950\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6317\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4966\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4184\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5250\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5182\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3620\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5226\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2386\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4412\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3195\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3642\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3208\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7035\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6355\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3750\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3791\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3703\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3282\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3771\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3239\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6059\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2215\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5406\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5822\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3808\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4569\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5236\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4941\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3247\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3880\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4263\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3665\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5930\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6059\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5713\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4572\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3749\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4425\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6880\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3873\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 2.3873\n",
      "5/5 [==============================] - 0s 997us/step - loss: 2.8892\n",
      "loss :  2.889216661453247\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[12.756906 ]\n",
      " [24.445395 ]\n",
      " [43.529594 ]\n",
      " [15.34151  ]\n",
      " [26.04903  ]\n",
      " [44.335022 ]\n",
      " [27.291645 ]\n",
      " [13.164069 ]\n",
      " [17.92514  ]\n",
      " [23.553722 ]\n",
      " [23.23053  ]\n",
      " [21.365257 ]\n",
      " [16.52159  ]\n",
      " [27.867308 ]\n",
      " [21.496984 ]\n",
      " [19.522934 ]\n",
      " [20.875908 ]\n",
      " [32.64931  ]\n",
      " [19.708675 ]\n",
      " [12.942666 ]\n",
      " [16.342148 ]\n",
      " [24.850845 ]\n",
      " [39.200413 ]\n",
      " [45.43506  ]\n",
      " [45.514076 ]\n",
      " [21.211231 ]\n",
      " [13.597031 ]\n",
      " [21.553123 ]\n",
      " [21.223204 ]\n",
      " [14.735618 ]\n",
      " [22.861506 ]\n",
      " [34.143692 ]\n",
      " [10.360172 ]\n",
      " [20.762596 ]\n",
      " [21.500757 ]\n",
      " [24.960999 ]\n",
      " [24.007452 ]\n",
      " [13.083759 ]\n",
      " [16.559507 ]\n",
      " [45.108288 ]\n",
      " [26.468796 ]\n",
      " [18.377577 ]\n",
      " [17.592632 ]\n",
      " [52.59298  ]\n",
      " [16.450182 ]\n",
      " [22.977161 ]\n",
      " [21.60262  ]\n",
      " [20.81184  ]\n",
      " [18.305567 ]\n",
      " [17.972763 ]\n",
      " [34.754772 ]\n",
      " [22.018766 ]\n",
      " [21.743454 ]\n",
      " [11.655267 ]\n",
      " [17.093641 ]\n",
      " [13.902489 ]\n",
      " [13.413226 ]\n",
      " [ 8.794134 ]\n",
      " [30.543896 ]\n",
      " [12.758253 ]\n",
      " [18.103554 ]\n",
      " [20.456799 ]\n",
      " [13.464076 ]\n",
      " [19.97958  ]\n",
      " [21.262747 ]\n",
      " [22.734665 ]\n",
      " [23.708437 ]\n",
      " [15.353795 ]\n",
      " [24.328842 ]\n",
      " [27.922426 ]\n",
      " [18.50495  ]\n",
      " [24.3418   ]\n",
      " [17.91117  ]\n",
      " [23.751293 ]\n",
      " [14.667802 ]\n",
      " [20.735842 ]\n",
      " [ 9.229185 ]\n",
      " [20.175194 ]\n",
      " [26.107344 ]\n",
      " [ 8.824868 ]\n",
      " [25.723116 ]\n",
      " [ 8.915626 ]\n",
      " [23.087795 ]\n",
      " [20.22229  ]\n",
      " [17.730717 ]\n",
      " [23.39706  ]\n",
      " [13.766001 ]\n",
      " [21.552101 ]\n",
      " [20.327658 ]\n",
      " [29.44783  ]\n",
      " [14.665525 ]\n",
      " [31.055729 ]\n",
      " [20.959883 ]\n",
      " [24.10193  ]\n",
      " [25.522573 ]\n",
      " [25.0733   ]\n",
      " [13.131645 ]\n",
      " [29.978992 ]\n",
      " [22.033676 ]\n",
      " [35.397858 ]\n",
      " [20.22688  ]\n",
      " [12.29889  ]\n",
      " [44.72958  ]\n",
      " [13.089239 ]\n",
      " [19.712135 ]\n",
      " [22.30271  ]\n",
      " [16.687391 ]\n",
      " [13.795523 ]\n",
      " [20.785109 ]\n",
      " [21.55877  ]\n",
      " [20.375378 ]\n",
      " [22.01643  ]\n",
      " [14.341048 ]\n",
      " [17.953102 ]\n",
      " [14.710992 ]\n",
      " [23.285831 ]\n",
      " [30.65851  ]\n",
      " [11.0391245]\n",
      " [17.927284 ]\n",
      " [19.417557 ]\n",
      " [20.29543  ]\n",
      " [28.951008 ]\n",
      " [17.509438 ]\n",
      " [15.284762 ]\n",
      " [16.626717 ]\n",
      " [19.618698 ]\n",
      " [19.91865  ]\n",
      " [22.64183  ]\n",
      " [14.716538 ]\n",
      " [20.485947 ]\n",
      " [19.748405 ]\n",
      " [10.38456  ]\n",
      " [14.226094 ]\n",
      " [ 8.187853 ]\n",
      " [14.191997 ]\n",
      " [15.122171 ]\n",
      " [18.076843 ]\n",
      " [12.064831 ]\n",
      " [18.797089 ]\n",
      " [21.328371 ]\n",
      " [19.038248 ]\n",
      " [23.770359 ]\n",
      " [12.786847 ]\n",
      " [21.91323  ]\n",
      " [49.612885 ]\n",
      " [13.150688 ]\n",
      " [20.370605 ]\n",
      " [19.332956 ]\n",
      " [22.793394 ]\n",
      " [17.155281 ]\n",
      " [ 9.155775 ]\n",
      " [20.079338 ]]\n",
      "=================\n",
      "R2 :  0.7589945575802554\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4385\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3016\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5874\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4025\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3690\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2898\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5147\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2816\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5075\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6019\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4733\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4561\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6276\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3469\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4802\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7849\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3599\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1997\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4561\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1721\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3213\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3971\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4021\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3530\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8717\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3342\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6424\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3763\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4442\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2564\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4402\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4736\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4546\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3407\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3541\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2477\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2410\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3151\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2778\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2904\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3288\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6437\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4740\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5097\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4632\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4776\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3448\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8067\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4717\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4235\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5670\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3816\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3288\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2908\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5444\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3961\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3477\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2217\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4474\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4599\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3387\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3511\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3609\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3270\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4557\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4264\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2472\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3771\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4901\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2495\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2763\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2016\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2244\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3425\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3335\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5659\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2990\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5830\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2613\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2196\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4449\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3290\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4741\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3285\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4143\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3151\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3833\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3612\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3774\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 130us/step - loss: 2.3774\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.8438\n",
      "loss :  2.843778610229492\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[12.91848  ]\n",
      " [26.074404 ]\n",
      " [46.417866 ]\n",
      " [13.620531 ]\n",
      " [28.277727 ]\n",
      " [47.747654 ]\n",
      " [30.10198  ]\n",
      " [11.955917 ]\n",
      " [17.646355 ]\n",
      " [24.885778 ]\n",
      " [24.76583  ]\n",
      " [20.82618  ]\n",
      " [13.995682 ]\n",
      " [34.820724 ]\n",
      " [20.506416 ]\n",
      " [19.550781 ]\n",
      " [20.30467  ]\n",
      " [38.679085 ]\n",
      " [19.88714  ]\n",
      " [11.434854 ]\n",
      " [14.192328 ]\n",
      " [23.42156  ]\n",
      " [43.32504  ]\n",
      " [48.099403 ]\n",
      " [48.894848 ]\n",
      " [21.308187 ]\n",
      " [12.500476 ]\n",
      " [21.381014 ]\n",
      " [20.880243 ]\n",
      " [14.597797 ]\n",
      " [22.977964 ]\n",
      " [35.892845 ]\n",
      " [ 8.8131075]\n",
      " [20.518904 ]\n",
      " [21.39953  ]\n",
      " [29.083149 ]\n",
      " [23.873854 ]\n",
      " [13.928719 ]\n",
      " [16.479633 ]\n",
      " [47.344784 ]\n",
      " [29.09683  ]\n",
      " [18.353483 ]\n",
      " [20.262354 ]\n",
      " [54.08209  ]\n",
      " [16.525026 ]\n",
      " [22.817215 ]\n",
      " [21.43039  ]\n",
      " [22.650137 ]\n",
      " [18.151667 ]\n",
      " [20.082811 ]\n",
      " [35.98953  ]\n",
      " [23.654749 ]\n",
      " [21.626907 ]\n",
      " [12.890442 ]\n",
      " [17.514347 ]\n",
      " [12.984724 ]\n",
      " [13.626118 ]\n",
      " [ 8.548507 ]\n",
      " [32.946053 ]\n",
      " [13.171635 ]\n",
      " [18.249392 ]\n",
      " [20.58541  ]\n",
      " [13.44368  ]\n",
      " [19.282082 ]\n",
      " [21.0633   ]\n",
      " [23.658499 ]\n",
      " [23.55221  ]\n",
      " [14.052683 ]\n",
      " [24.070208 ]\n",
      " [30.382315 ]\n",
      " [18.6017   ]\n",
      " [27.733187 ]\n",
      " [17.83328  ]\n",
      " [23.56869  ]\n",
      " [13.451528 ]\n",
      " [19.755804 ]\n",
      " [10.324808 ]\n",
      " [19.548079 ]\n",
      " [28.788666 ]\n",
      " [ 8.617205 ]\n",
      " [29.15833  ]\n",
      " [10.1333275]\n",
      " [23.016027 ]\n",
      " [19.698114 ]\n",
      " [17.758747 ]\n",
      " [23.763128 ]\n",
      " [16.491545 ]\n",
      " [21.44952  ]\n",
      " [19.999004 ]\n",
      " [31.802443 ]\n",
      " [14.07976  ]\n",
      " [33.56654  ]\n",
      " [22.136679 ]\n",
      " [23.820494 ]\n",
      " [27.838701 ]\n",
      " [24.943754 ]\n",
      " [13.616633 ]\n",
      " [31.966692 ]\n",
      " [24.429861 ]\n",
      " [37.406017 ]\n",
      " [20.175924 ]\n",
      " [10.485331 ]\n",
      " [47.480724 ]\n",
      " [13.419434 ]\n",
      " [19.686842 ]\n",
      " [23.723066 ]\n",
      " [18.136377 ]\n",
      " [12.521801 ]\n",
      " [18.641975 ]\n",
      " [21.68978  ]\n",
      " [21.72001  ]\n",
      " [21.20552  ]\n",
      " [15.324488 ]\n",
      " [18.182245 ]\n",
      " [14.0402975]\n",
      " [25.172495 ]\n",
      " [32.841633 ]\n",
      " [ 9.2081375]\n",
      " [18.237452 ]\n",
      " [19.53948  ]\n",
      " [21.905008 ]\n",
      " [32.397068 ]\n",
      " [17.39736  ]\n",
      " [12.392628 ]\n",
      " [16.41423  ]\n",
      " [20.339958 ]\n",
      " [20.5381   ]\n",
      " [22.32908  ]\n",
      " [15.900091 ]\n",
      " [17.36153  ]\n",
      " [19.814837 ]\n",
      " [10.497666 ]\n",
      " [13.814398 ]\n",
      " [ 8.604488 ]\n",
      " [13.493423 ]\n",
      " [14.079209 ]\n",
      " [18.110376 ]\n",
      " [12.817937 ]\n",
      " [18.697794 ]\n",
      " [20.42993  ]\n",
      " [19.671946 ]\n",
      " [25.08983  ]\n",
      " [11.734932 ]\n",
      " [21.47352  ]\n",
      " [52.881367 ]\n",
      " [13.411936 ]\n",
      " [21.13739  ]\n",
      " [18.674646 ]\n",
      " [24.733412 ]\n",
      " [17.063908 ]\n",
      " [ 8.781413 ]\n",
      " [20.446436 ]]\n",
      "=================\n",
      "R2 :  0.7543371034240414\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3372\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3231\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4888\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1835\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1980\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3321\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2854\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3042\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1737\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4052\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2557\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3690\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3805\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5700\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3446\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2532\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2238\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2500\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2752\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0728\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3355\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2585\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2818\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2201\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4755\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2584\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2725\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1415\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4289\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0989\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2885\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2219\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1520\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4176\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0729\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2282\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3919\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5814\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3724\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3446\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1198\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3588\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3729\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3124\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2590\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4165\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4875\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3660\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3196\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3612\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2398\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6791\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2710\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3259\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3651\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3361\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1584\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3078\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2350\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2416\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1404\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2532\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2454\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3980\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1619\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4045\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2026\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1836\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2241\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2099\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0333\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3091\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1377\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2934\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4754\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1988\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3175\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1950\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1803\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3888\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1466\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3663\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3806\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1553\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2712\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9613\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3815\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3259\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0938\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 130us/step - loss: 2.0938\n",
      "5/5 [==============================] - 0s 997us/step - loss: 3.0990\n",
      "loss :  3.0990471839904785\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[13.798507 ]\n",
      " [25.657944 ]\n",
      " [46.94806  ]\n",
      " [15.426822 ]\n",
      " [27.484772 ]\n",
      " [51.83623  ]\n",
      " [29.515045 ]\n",
      " [11.937392 ]\n",
      " [17.756454 ]\n",
      " [25.210087 ]\n",
      " [25.360424 ]\n",
      " [21.108997 ]\n",
      " [18.458134 ]\n",
      " [36.785046 ]\n",
      " [21.21043  ]\n",
      " [19.963478 ]\n",
      " [20.957508 ]\n",
      " [36.013393 ]\n",
      " [19.552135 ]\n",
      " [12.434931 ]\n",
      " [16.435871 ]\n",
      " [33.00004  ]\n",
      " [44.769848 ]\n",
      " [49.451115 ]\n",
      " [54.45975  ]\n",
      " [21.821293 ]\n",
      " [13.359053 ]\n",
      " [22.150295 ]\n",
      " [21.644161 ]\n",
      " [13.265349 ]\n",
      " [23.812645 ]\n",
      " [35.760906 ]\n",
      " [12.003526 ]\n",
      " [21.216175 ]\n",
      " [22.209854 ]\n",
      " [27.925777 ]\n",
      " [24.426418 ]\n",
      " [13.802724 ]\n",
      " [16.397068 ]\n",
      " [46.70103  ]\n",
      " [28.966938 ]\n",
      " [18.629383 ]\n",
      " [17.877987 ]\n",
      " [53.67589  ]\n",
      " [16.586258 ]\n",
      " [23.44115  ]\n",
      " [22.182396 ]\n",
      " [22.587936 ]\n",
      " [18.236637 ]\n",
      " [26.818237 ]\n",
      " [37.46699  ]\n",
      " [23.28481  ]\n",
      " [22.390202 ]\n",
      " [11.863686 ]\n",
      " [22.740238 ]\n",
      " [15.01163  ]\n",
      " [13.445924 ]\n",
      " [ 8.393316 ]\n",
      " [33.89075  ]\n",
      " [12.483335 ]\n",
      " [18.593666 ]\n",
      " [20.978855 ]\n",
      " [13.522098 ]\n",
      " [19.646996 ]\n",
      " [21.954765 ]\n",
      " [24.226582 ]\n",
      " [23.889458 ]\n",
      " [14.418767 ]\n",
      " [24.784306 ]\n",
      " [30.057892 ]\n",
      " [18.77945  ]\n",
      " [27.387524 ]\n",
      " [17.9141   ]\n",
      " [23.796474 ]\n",
      " [15.0777445]\n",
      " [20.09956  ]\n",
      " [12.753906 ]\n",
      " [20.053822 ]\n",
      " [28.058472 ]\n",
      " [ 8.393316 ]\n",
      " [29.179947 ]\n",
      " [12.584289 ]\n",
      " [23.693077 ]\n",
      " [20.358776 ]\n",
      " [18.543026 ]\n",
      " [23.87289  ]\n",
      " [17.284855 ]\n",
      " [22.278004 ]\n",
      " [20.738766 ]\n",
      " [31.4719   ]\n",
      " [14.51902  ]\n",
      " [33.457905 ]\n",
      " [21.496078 ]\n",
      " [24.813225 ]\n",
      " [27.19437  ]\n",
      " [26.1274   ]\n",
      " [12.994472 ]\n",
      " [33.612457 ]\n",
      " [25.404182 ]\n",
      " [37.93054  ]\n",
      " [20.818722 ]\n",
      " [12.323234 ]\n",
      " [46.661232 ]\n",
      " [12.508947 ]\n",
      " [20.048923 ]\n",
      " [24.1988   ]\n",
      " [17.938187 ]\n",
      " [13.701334 ]\n",
      " [22.31475  ]\n",
      " [22.47749  ]\n",
      " [21.204502 ]\n",
      " [21.688133 ]\n",
      " [15.330628 ]\n",
      " [19.35384  ]\n",
      " [14.863081 ]\n",
      " [24.760063 ]\n",
      " [32.84007  ]\n",
      " [12.37832  ]\n",
      " [18.928284 ]\n",
      " [18.413403 ]\n",
      " [29.481853 ]\n",
      " [31.029396 ]\n",
      " [17.80085  ]\n",
      " [12.447171 ]\n",
      " [16.320156 ]\n",
      " [20.346664 ]\n",
      " [20.636639 ]\n",
      " [22.682953 ]\n",
      " [16.974297 ]\n",
      " [22.608448 ]\n",
      " [20.17237  ]\n",
      " [12.28546  ]\n",
      " [13.560657 ]\n",
      " [ 9.670089 ]\n",
      " [14.014808 ]\n",
      " [13.902141 ]\n",
      " [18.818382 ]\n",
      " [12.515773 ]\n",
      " [18.874996 ]\n",
      " [20.880108 ]\n",
      " [20.012276 ]\n",
      " [24.341549 ]\n",
      " [12.00747  ]\n",
      " [22.263493 ]\n",
      " [57.898785 ]\n",
      " [12.522475 ]\n",
      " [20.642982 ]\n",
      " [19.363747 ]\n",
      " [24.981178 ]\n",
      " [22.92789  ]\n",
      " [12.952163 ]\n",
      " [20.825535 ]]\n",
      "=================\n",
      "R2 :  0.7355541860581083\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5991\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2287\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5912\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0937\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4467\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0857\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2158\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2935\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1875\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3126\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1613\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1830\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3801\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0965\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1957\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1544\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1584\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2198\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2479\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3677\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2868\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2704\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9318\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1723\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3996\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1087\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1332\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1445\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0768\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3802\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2117\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1971\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2724\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2221\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1706\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3556\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1976\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3669\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2640\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1764\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0965\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1993\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6688\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0339\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1495\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9865\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3242\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1593\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4262\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3332\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1895\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1295\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0020\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1488\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2450\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1818\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1274\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3335\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4219\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0704\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1725\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3588\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3224\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4541\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0917\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1599\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3754\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9890\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1578\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6507\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2199\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2626\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1327\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0547\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1234\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1414\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0166\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0622\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2456\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0541\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1504\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4690\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4003\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1407\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2678\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0069\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2421\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0678\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1939\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 110us/step - loss: 2.1939\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.8054\n",
      "loss :  2.805398941040039\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[12.019251 ]\n",
      " [26.13828  ]\n",
      " [51.276897 ]\n",
      " [12.037084 ]\n",
      " [27.864862 ]\n",
      " [43.785877 ]\n",
      " [28.909288 ]\n",
      " [11.91796  ]\n",
      " [17.755333 ]\n",
      " [26.214176 ]\n",
      " [24.71462  ]\n",
      " [20.782919 ]\n",
      " [13.541481 ]\n",
      " [29.996153 ]\n",
      " [20.98102  ]\n",
      " [19.939734 ]\n",
      " [20.658907 ]\n",
      " [37.58849  ]\n",
      " [19.585226 ]\n",
      " [11.91796  ]\n",
      " [14.55376  ]\n",
      " [22.556671 ]\n",
      " [46.40563  ]\n",
      " [48.867405 ]\n",
      " [47.13888  ]\n",
      " [21.485415 ]\n",
      " [11.91796  ]\n",
      " [21.736338 ]\n",
      " [21.534967 ]\n",
      " [12.729835 ]\n",
      " [23.436558 ]\n",
      " [36.030773 ]\n",
      " [11.4980345]\n",
      " [20.751673 ]\n",
      " [21.841127 ]\n",
      " [28.86266  ]\n",
      " [25.269663 ]\n",
      " [12.081542 ]\n",
      " [16.531212 ]\n",
      " [44.59776  ]\n",
      " [29.020697 ]\n",
      " [18.648964 ]\n",
      " [16.51472  ]\n",
      " [50.569515 ]\n",
      " [16.701273 ]\n",
      " [23.833635 ]\n",
      " [21.824034 ]\n",
      " [21.984537 ]\n",
      " [18.229286 ]\n",
      " [20.856745 ]\n",
      " [36.906822 ]\n",
      " [23.905817 ]\n",
      " [22.048891 ]\n",
      " [11.509012 ]\n",
      " [18.453407 ]\n",
      " [11.91796  ]\n",
      " [14.0084   ]\n",
      " [ 8.33088  ]\n",
      " [33.481396 ]\n",
      " [11.91796  ]\n",
      " [18.642204 ]\n",
      " [20.728434 ]\n",
      " [13.921685 ]\n",
      " [19.77522  ]\n",
      " [21.590666 ]\n",
      " [23.643335 ]\n",
      " [24.541313 ]\n",
      " [14.753963 ]\n",
      " [25.688255 ]\n",
      " [29.927813 ]\n",
      " [18.863108 ]\n",
      " [26.717718 ]\n",
      " [17.96064  ]\n",
      " [24.61892  ]\n",
      " [12.941718 ]\n",
      " [20.075323 ]\n",
      " [11.427716 ]\n",
      " [19.855965 ]\n",
      " [26.94701  ]\n",
      " [ 8.33088  ]\n",
      " [27.072523 ]\n",
      " [11.403415 ]\n",
      " [23.812433 ]\n",
      " [20.203642 ]\n",
      " [18.193619 ]\n",
      " [24.506289 ]\n",
      " [13.989875 ]\n",
      " [21.831627 ]\n",
      " [20.551207 ]\n",
      " [31.137072 ]\n",
      " [14.009761 ]\n",
      " [33.234127 ]\n",
      " [21.902178 ]\n",
      " [25.624445 ]\n",
      " [27.587439 ]\n",
      " [27.069025 ]\n",
      " [11.91796  ]\n",
      " [33.169655 ]\n",
      " [23.207247 ]\n",
      " [38.516193 ]\n",
      " [20.580317 ]\n",
      " [11.661344 ]\n",
      " [50.633533 ]\n",
      " [11.91796  ]\n",
      " [19.924547 ]\n",
      " [24.102736 ]\n",
      " [17.270067 ]\n",
      " [11.91796  ]\n",
      " [21.063845 ]\n",
      " [22.1225   ]\n",
      " [20.975899 ]\n",
      " [21.36996  ]\n",
      " [14.72804  ]\n",
      " [18.631351 ]\n",
      " [13.764092 ]\n",
      " [24.1676   ]\n",
      " [32.69352  ]\n",
      " [11.7031145]\n",
      " [18.79983  ]\n",
      " [18.912643 ]\n",
      " [19.578306 ]\n",
      " [31.99511  ]\n",
      " [17.763628 ]\n",
      " [11.91796  ]\n",
      " [16.436953 ]\n",
      " [20.301168 ]\n",
      " [20.519928 ]\n",
      " [23.217793 ]\n",
      " [16.515387 ]\n",
      " [17.83821  ]\n",
      " [20.020952 ]\n",
      " [11.652934 ]\n",
      " [12.831141 ]\n",
      " [ 8.33088  ]\n",
      " [13.508233 ]\n",
      " [14.645846 ]\n",
      " [18.298052 ]\n",
      " [11.8690405]\n",
      " [18.856907 ]\n",
      " [20.865143 ]\n",
      " [20.025915 ]\n",
      " [24.982187 ]\n",
      " [11.884908 ]\n",
      " [21.78077  ]\n",
      " [49.4716   ]\n",
      " [11.91796  ]\n",
      " [21.052496 ]\n",
      " [19.299658 ]\n",
      " [23.949343 ]\n",
      " [18.21843  ]\n",
      " [ 8.886878 ]\n",
      " [20.684603 ]]\n",
      "=================\n",
      "R2 :  0.7624907077014464\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0708\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0884\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4944\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1644\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2224\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3226\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0622\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2071\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1577\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0491\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1776\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6094\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4699\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1546\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.9415\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0270\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0720\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9916\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3138\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2990\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4960\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0888\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2431\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2365\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4133\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1519\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4195\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0365\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0624\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4499\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3242\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2482\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1724\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0823\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2560\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0813\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0386\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5450\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1661\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1930\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2821\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1559\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0975\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0201\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1998\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1060\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0857\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8208\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1659\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2749\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1845\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4826\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2855\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0612\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1136\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1377\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0651\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1408\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.8464\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1959\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1343\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2776\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0280\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0774\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2076\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.9727\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1922\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.8652\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.2670\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.1221\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0198\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0547\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.9991\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.3754\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.0240\n",
      "Epoch 76/100\n",
      "  1/100 [..............................] - ETA: 0s - loss: 2.1311"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12108\\3548245157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\study\\keras\\\\boston.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "now = datetime.now()\n",
    "\n",
    "model.compile(loss=\"mae\",optimizer=\"adam\")\n",
    "f = open(\"C:\\study\\keras\\\\boston.txt\",'a')\n",
    "while (True):\n",
    "    model.fit(x_train,y_train,epochs=100,batch_size=4,steps_per_epoch=100)\n",
    "    loss = model.evaluate(x_test,y_test)\n",
    "    print(\"loss : \",loss)\n",
    "    y_predict =model.predict(x_test)\n",
    "    print(\"=================\")\n",
    "    print(y_test)\n",
    "    print(y_predict)\n",
    "    print(\"=================\")\n",
    "    r2 = r2_score(y_test,y_predict)\n",
    "    print(\"R2 : \",r2)\n",
    "\n",
    "    f.write(str(datetime.now())+str(r2)+\"\\n\") \n",
    "    \n",
    "    if r2 >= 0.8 :\n",
    "        model.save(\"boston.h5\")\n",
    "        f.write(str(datetime.now())+str(r2)+\"\\n\") \n",
    "        f.close()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
