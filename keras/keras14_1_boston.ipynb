{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n",
      "2.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "print(sk.__version__)\n",
    "print(tf.__version__)\n",
    "dataset = load_boston()\n",
    "x = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# 13개의 칼럼 을 가지고있는 데이터를 조회합니다\n",
    "print(x.shape) #(506,13)\n",
    "# print(x)\n",
    "print(y.shape) #(506,)\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# 사이킥 런의 칼럼 조회기능\n",
    "print(dataset.feature_names)\n",
    "print(len(dataset.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사이킥런의 데이터 요약\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델구성\n",
    "# 실습 train 0.7 이상\n",
    "# 평가지표 R2 : 0.8 이상으로 뽑아내보기 / RMSE 사용\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(x,y,\n",
    "    train_size=0.7,\n",
    "    shuffle = True,\n",
    "    random_state= 123\n",
    ")\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 10)                140       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,911\n",
      "Trainable params: 1,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "number1 = randrange(1,10,1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10,input_dim = 13),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(10,activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 1ms/step - loss: 19.7428\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.9478\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.6417\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.9596\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.2153\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.0772\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.2711\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.1108\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.3753\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.9699\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.2275\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.9624\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.1230\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.0159\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8040\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.9302\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 5.2262\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.5197\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.7047\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.3715\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.8875\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.5038\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.4175\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.7517\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.5374\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.6279\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.6417\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.2261\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4211\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.4700\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.1101\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.5817\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.2299\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.2192\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.0011\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.1779\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.0834\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.0834\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.1363\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7839\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.0182\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.5849\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.8307\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.8218\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 4.1611\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5441\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6998\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.9839\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.8962\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6196\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6530\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6722\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4959\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5571\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.7601\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5493\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3424\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2719\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.6427\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5598\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.7312\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.4276\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.3103\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1858\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1000\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3654\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2648\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3330\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.5682\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2325\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2893\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1298\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2829\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.0945\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3522\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2654\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3222\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0474\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0559\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2816\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1937\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1183\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2378\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1651\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2619\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1695\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3633\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3935\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2524\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 3.2524\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.5970\n",
      "loss :  3.5970406532287598\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[11.246447 ]\n",
      " [23.052729 ]\n",
      " [35.755184 ]\n",
      " [13.989231 ]\n",
      " [25.099997 ]\n",
      " [32.818817 ]\n",
      " [23.443815 ]\n",
      " [11.13975  ]\n",
      " [17.37882  ]\n",
      " [22.380636 ]\n",
      " [22.646955 ]\n",
      " [21.338736 ]\n",
      " [ 8.894084 ]\n",
      " [26.288668 ]\n",
      " [18.585384 ]\n",
      " [19.845552 ]\n",
      " [20.051186 ]\n",
      " [31.097319 ]\n",
      " [19.463892 ]\n",
      " [12.552216 ]\n",
      " [ 9.713091 ]\n",
      " [20.468513 ]\n",
      " [28.939337 ]\n",
      " [38.497856 ]\n",
      " [34.119396 ]\n",
      " [20.625639 ]\n",
      " [14.148299 ]\n",
      " [20.86736  ]\n",
      " [19.991547 ]\n",
      " [ 9.95204  ]\n",
      " [22.39953  ]\n",
      " [31.979898 ]\n",
      " [10.560935 ]\n",
      " [20.065027 ]\n",
      " [21.059225 ]\n",
      " [22.504644 ]\n",
      " [23.78784  ]\n",
      " [13.33342  ]\n",
      " [15.55151  ]\n",
      " [38.241943 ]\n",
      " [23.660765 ]\n",
      " [17.906206 ]\n",
      " [16.008396 ]\n",
      " [44.772358 ]\n",
      " [15.3881   ]\n",
      " [21.984362 ]\n",
      " [21.428484 ]\n",
      " [22.348196 ]\n",
      " [18.394102 ]\n",
      " [15.409661 ]\n",
      " [33.384468 ]\n",
      " [20.831215 ]\n",
      " [21.514627 ]\n",
      " [12.103104 ]\n",
      " [16.85321  ]\n",
      " [13.68842  ]\n",
      " [13.049106 ]\n",
      " [ 9.6278925]\n",
      " [24.8551   ]\n",
      " [12.942549 ]\n",
      " [19.035042 ]\n",
      " [21.93243  ]\n",
      " [ 9.722915 ]\n",
      " [15.497157 ]\n",
      " [21.496044 ]\n",
      " [22.569431 ]\n",
      " [22.904459 ]\n",
      " [15.338239 ]\n",
      " [23.255047 ]\n",
      " [25.008034 ]\n",
      " [18.637356 ]\n",
      " [22.44242  ]\n",
      " [17.82961  ]\n",
      " [22.772158 ]\n",
      " [15.208683 ]\n",
      " [15.489306 ]\n",
      " [10.449238 ]\n",
      " [19.662876 ]\n",
      " [27.557285 ]\n",
      " [ 8.79465  ]\n",
      " [24.903627 ]\n",
      " [10.508639 ]\n",
      " [22.727833 ]\n",
      " [20.050093 ]\n",
      " [17.237848 ]\n",
      " [22.81271  ]\n",
      " [14.844404 ]\n",
      " [20.650198 ]\n",
      " [20.585018 ]\n",
      " [25.304056 ]\n",
      " [12.24372  ]\n",
      " [26.197077 ]\n",
      " [20.539797 ]\n",
      " [22.982115 ]\n",
      " [24.570833 ]\n",
      " [23.752302 ]\n",
      " [13.122095 ]\n",
      " [27.158419 ]\n",
      " [21.577797 ]\n",
      " [30.30831  ]\n",
      " [19.739214 ]\n",
      " [11.819863 ]\n",
      " [36.15682  ]\n",
      " [12.916681 ]\n",
      " [20.00082  ]\n",
      " [21.201857 ]\n",
      " [16.243513 ]\n",
      " [12.789491 ]\n",
      " [17.717554 ]\n",
      " [20.813004 ]\n",
      " [20.571247 ]\n",
      " [20.976889 ]\n",
      " [16.167389 ]\n",
      " [18.927692 ]\n",
      " [12.098917 ]\n",
      " [22.90323  ]\n",
      " [27.185423 ]\n",
      " [11.162115 ]\n",
      " [18.09175  ]\n",
      " [17.130959 ]\n",
      " [20.818811 ]\n",
      " [29.896046 ]\n",
      " [14.58819  ]\n",
      " [11.930712 ]\n",
      " [15.721927 ]\n",
      " [21.055124 ]\n",
      " [18.907957 ]\n",
      " [22.503649 ]\n",
      " [15.780123 ]\n",
      " [13.260892 ]\n",
      " [19.442884 ]\n",
      " [11.219286 ]\n",
      " [13.609062 ]\n",
      " [ 9.486164 ]\n",
      " [ 9.506807 ]\n",
      " [13.923461 ]\n",
      " [18.275509 ]\n",
      " [12.212172 ]\n",
      " [19.188278 ]\n",
      " [18.04983  ]\n",
      " [20.14672  ]\n",
      " [20.453915 ]\n",
      " [11.70431  ]\n",
      " [21.581804 ]\n",
      " [38.34091  ]\n",
      " [13.479849 ]\n",
      " [20.327423 ]\n",
      " [17.623213 ]\n",
      " [22.271688 ]\n",
      " [16.309843 ]\n",
      " [12.376335 ]\n",
      " [18.581102 ]]\n",
      "=================\n",
      "R2 :  0.6575151296549455\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3328\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2540\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0235\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2387\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9326\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8600\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3358\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0666\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9755\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1899\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9137\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9484\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2887\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8702\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3569\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9280\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1859\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1536\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3155\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0235\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2832\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9377\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1262\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8773\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2295\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9821\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9764\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9508\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9142\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1026\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1715\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.4965\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8880\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1398\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0100\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2067\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9517\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7327\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7078\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0607\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8577\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9603\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9305\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9572\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9010\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9939\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9990\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7554\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0787\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.2881\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8998\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9785\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1675\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8834\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6305\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0075\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8926\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9810\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6027\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0452\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7715\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7806\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.3140\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9099\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9878\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8548\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7030\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7719\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9469\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7115\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8660\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7544\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0147\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9048\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.1179\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8528\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9457\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9184\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7437\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8561\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8678\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7140\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5828\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0229\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.5735\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.4221\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0539\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9893\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8502\n",
      "Epoch 90/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: 2.8502\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.1569\n",
      "loss :  3.1569104194641113\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "=================\n",
      "[15.  26.6 45.4 20.8 34.9 21.9 28.7  7.2 20.  32.2 24.1 18.5 13.5 27.\n",
      " 23.1 18.9 24.5 43.1 19.8 13.8 15.6 50.  37.2 46.  50.  21.2 14.9 19.6\n",
      " 19.4 18.6 26.5 32.  10.9 20.  21.4 31.  25.  15.4 13.1 37.6 37.  18.9\n",
      " 27.9 50.  14.4 22.  19.9 21.6 15.6 15.  32.4 29.6 20.4 12.3 19.1 14.9\n",
      " 17.8  8.8 35.4 11.5 19.6 20.6 15.6 19.9 23.3 22.3 24.8 16.1 22.8 30.5\n",
      " 20.4 24.4 16.6 26.2 16.4 20.1 13.9 19.4 22.8 13.8 31.6 10.5 23.8 22.4\n",
      " 19.3 22.2 12.6 19.4 22.2 29.8  9.6 34.9 21.4 25.3 32.9 26.6 14.6 31.5\n",
      " 23.3 33.3 17.5 19.1 48.5 17.1 23.1 28.4 18.9 13.  17.2 24.1 18.5 21.8\n",
      " 13.3 23.  14.1 23.9 24.  17.2 21.5 19.1 20.8 36.  20.1  8.7 13.6 22.\n",
      " 22.2 21.1 13.4 17.4 20.1 10.2 23.1 10.2 13.1 14.3 14.5  7.2 19.6 20.6\n",
      " 22.7 26.4  7.5 20.3 50.   8.5 20.3 16.1 22.  19.6 10.2 23.2]\n",
      "[[12.190877 ]\n",
      " [26.493315 ]\n",
      " [44.936417 ]\n",
      " [16.749329 ]\n",
      " [26.30585  ]\n",
      " [50.976593 ]\n",
      " [29.213717 ]\n",
      " [11.905076 ]\n",
      " [19.256645 ]\n",
      " [24.786844 ]\n",
      " [24.489252 ]\n",
      " [23.016603 ]\n",
      " [11.311422 ]\n",
      " [35.67285  ]\n",
      " [21.157503 ]\n",
      " [21.221024 ]\n",
      " [21.969656 ]\n",
      " [39.116447 ]\n",
      " [21.019335 ]\n",
      " [14.186102 ]\n",
      " [12.696135 ]\n",
      " [24.688108 ]\n",
      " [37.514706 ]\n",
      " [48.158527 ]\n",
      " [50.94753  ]\n",
      " [22.773111 ]\n",
      " [16.546745 ]\n",
      " [22.901865 ]\n",
      " [22.368689 ]\n",
      " [11.777441 ]\n",
      " [24.380156 ]\n",
      " [38.872578 ]\n",
      " [11.916916 ]\n",
      " [21.78427  ]\n",
      " [23.044485 ]\n",
      " [27.699087 ]\n",
      " [25.96092  ]\n",
      " [14.350591 ]\n",
      " [17.360382 ]\n",
      " [47.24657  ]\n",
      " [26.874147 ]\n",
      " [19.351334 ]\n",
      " [19.585043 ]\n",
      " [57.50867  ]\n",
      " [17.311794 ]\n",
      " [24.26505  ]\n",
      " [22.931479 ]\n",
      " [24.28404  ]\n",
      " [19.664354 ]\n",
      " [17.142017 ]\n",
      " [40.491863 ]\n",
      " [23.015913 ]\n",
      " [22.94643  ]\n",
      " [12.259417 ]\n",
      " [18.942022 ]\n",
      " [16.0317   ]\n",
      " [15.088242 ]\n",
      " [11.739738 ]\n",
      " [31.307848 ]\n",
      " [13.019089 ]\n",
      " [20.159664 ]\n",
      " [23.076498 ]\n",
      " [11.284478 ]\n",
      " [17.46622  ]\n",
      " [23.182405 ]\n",
      " [24.66896  ]\n",
      " [25.244806 ]\n",
      " [14.4119625]\n",
      " [25.576101 ]\n",
      " [29.770868 ]\n",
      " [20.105667 ]\n",
      " [24.617477 ]\n",
      " [19.236784 ]\n",
      " [25.09291  ]\n",
      " [18.38112  ]\n",
      " [17.647589 ]\n",
      " [11.627127 ]\n",
      " [21.841185 ]\n",
      " [33.38989  ]\n",
      " [10.1071   ]\n",
      " [31.61255  ]\n",
      " [10.995426 ]\n",
      " [24.827814 ]\n",
      " [22.014685 ]\n",
      " [19.316782 ]\n",
      " [25.26364  ]\n",
      " [15.686705 ]\n",
      " [22.677248 ]\n",
      " [22.57913  ]\n",
      " [31.21657  ]\n",
      " [12.835203 ]\n",
      " [33.086445 ]\n",
      " [22.17497  ]\n",
      " [25.997242 ]\n",
      " [26.041735 ]\n",
      " [28.199574 ]\n",
      " [14.28058  ]\n",
      " [35.037636 ]\n",
      " [28.324373 ]\n",
      " [38.085873 ]\n",
      " [21.60845  ]\n",
      " [13.413015 ]\n",
      " [45.089413 ]\n",
      " [13.671238 ]\n",
      " [21.448757 ]\n",
      " [23.829319 ]\n",
      " [17.940205 ]\n",
      " [14.914476 ]\n",
      " [19.894747 ]\n",
      " [22.856009 ]\n",
      " [22.677507 ]\n",
      " [22.863554 ]\n",
      " [17.769953 ]\n",
      " [20.329824 ]\n",
      " [12.298962 ]\n",
      " [24.796427 ]\n",
      " [34.50294  ]\n",
      " [12.198842 ]\n",
      " [19.670593 ]\n",
      " [18.921518 ]\n",
      " [22.6499   ]\n",
      " [37.288624 ]\n",
      " [16.943382 ]\n",
      " [12.303823 ]\n",
      " [17.387777 ]\n",
      " [22.939957 ]\n",
      " [21.25495  ]\n",
      " [24.466703 ]\n",
      " [17.56392  ]\n",
      " [14.5257635]\n",
      " [21.38276  ]\n",
      " [11.619585 ]\n",
      " [12.749664 ]\n",
      " [10.776855 ]\n",
      " [11.31901  ]\n",
      " [13.007883 ]\n",
      " [19.411802 ]\n",
      " [12.429436 ]\n",
      " [20.357435 ]\n",
      " [20.488691 ]\n",
      " [22.022808 ]\n",
      " [22.34872  ]\n",
      " [12.146204 ]\n",
      " [23.501043 ]\n",
      " [54.26562  ]\n",
      " [13.770495 ]\n",
      " [22.027084 ]\n",
      " [20.005852 ]\n",
      " [24.40077  ]\n",
      " [18.178164 ]\n",
      " [12.27051  ]\n",
      " [20.887533 ]]\n",
      "=================\n",
      "R2 :  0.7123158448904022\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8445\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.0746\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.6819\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9673\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7732\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9629\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.7568\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7703\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.9917\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8476\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 2.8057\n",
      "Epoch 12/100\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.1952"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12108\\3611231641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\study\\keras\\\\boston.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bitcamp\\anaconda3\\envs\\tf27\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.metrics import r2_score\n",
    "now = datetime.now()\n",
    "\n",
    "model.compile(loss=\"mae\",optimizer=\"adam\")\n",
    "f = open(\"C:\\study\\keras\\\\boston.txt\",'a')\n",
    "while (True):\n",
    "    model.fit(x_train,y_train,epochs=100,batch_size=4,steps_per_epoch=100)\n",
    "    loss = model.evaluate(x_test,y_test)\n",
    "    print(\"loss : \",loss)\n",
    "    y_predict =model.predict(x_test)\n",
    "    print(\"=================\")\n",
    "    print(y_test)\n",
    "    print(y_predict)\n",
    "    print(\"=================\")\n",
    "    r2 = r2_score(y_test,y_predict)\n",
    "    print(\"R2 : \",r2)\n",
    "\n",
    "    f.write(str(now)+str(r2)+\"\\n\") \n",
    "    \n",
    "    if r2 >= 0.8 :\n",
    "        f.write(str(now)+str(r2)+\"\\n\") \n",
    "        model.save(\"boston.h5\")\n",
    "        f.cloes()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b47fb4e6c68d4941015efb0bbf71549277582fe8531338196fc3c7fa71b6aab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
